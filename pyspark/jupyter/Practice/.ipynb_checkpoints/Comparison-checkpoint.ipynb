{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = StructType([\n",
    "StructField(\"Name\", StringType(), True),\n",
    "StructField(\"Description\", StringType(), True),\n",
    "StructField(\"Primary_database_model\", StringType(), True),\n",
    "StructField(\"DB-Engines_Ranking \", StringType(), True),\n",
    "StructField(\"Trend_Chart\", StringType(), True),\n",
    "StructField(\"Website\", StringType(), True),\n",
    "StructField(\"Technical_documentation\", StringType(), True),\n",
    "StructField(\"Developer\", StringType(), True),\n",
    "StructField(\"Initial_release\", StringType(), True),\n",
    "StructField(\"Current_release\", StringType(), True),\n",
    "StructField(\"License\", StringType(), True),\n",
    "StructField(\"Cloud-based_only\", StringType(), True),\n",
    "StructField(\"Implementation_language\", StringType(), True),\n",
    "StructField(\"Server_operating_systems\", StringType(), True),\n",
    "StructField(\"Data_scheme\", StringType(), True),\n",
    "StructField(\"Typing\", StringType(), True),\n",
    "StructField(\"XML_support\", StringType(), True),\n",
    "StructField(\"Secondary_indexes\", StringType(), True),\n",
    "StructField(\"SQL\", StringType(), True),\n",
    "StructField(\"APIs_and_other_access_methods\", StringType(), True),\n",
    "StructField(\"Supported_programming_languages\", StringType(), True),\n",
    "StructField(\"Server-side_scripts \", StringType(), True),\n",
    "StructField(\"Triggers\", StringType(), True),\n",
    "StructField(\"Partitioning_methods\", StringType(), True),\n",
    "StructField(\"Replication_methods\", StringType(), True),\n",
    "StructField(\"MapReduce\", StringType(), True),\n",
    "StructField(\"Consistency_concepts\", StringType(), True),\n",
    "StructField(\"Foreign_keys\", StringType(), True),\n",
    "StructField(\"Transaction_concepts\", StringType(), True),\n",
    "StructField(\"Concurrency\", StringType(), True),\n",
    "StructField(\"Durability\", StringType(), True),\n",
    "StructField(\"In-memory_capabilities \", StringType(), True),\n",
    "StructField(\"User_concepts\", StringType(), True),\n",
    "StructField(\"Specific_characteristics\", StringType(), True),\n",
    "StructField(\"Competitive_advantages\", StringType(), True),\n",
    "StructField(\"Typical_application_scenarios\", StringType(), True),\n",
    "StructField(\"Key_customers\", StringType(), True),\n",
    "StructField(\"Market_metrics\", StringType(), True),\n",
    "StructField(\"Licensing_and_pricing_models\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, column \n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "dfInitial=spark.read.csv(\"C:/tmp/hive/initial.csv\",schema=mySchema)\n",
    "df=spark.read.format(\"csv\").option(\"inferSchema\", \"true\").load(\"Cassandra.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------------+-----+----+--------------------+--------------------+\n",
      "|       _1|                  _2|               _3|   _4|  _5|                  _6|                  _7|\n",
      "+---------+--------------------+-----------------+-----+----+--------------------+--------------------+\n",
      "|Cassandra|Wide-column store...|Wide column store|Score|Rank|Wide-column store...|cassandra.apache.org|\n",
      "+---------+--------------------+-----------------+-----+----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([(\"Cassandra\",\n",
    "df.where(col(\"_c0\") == \"Description\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Primary_database_model\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"DB-Engines_Ranking\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Trend_Chart\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Description\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Website\").select('_c2').head()[0])]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRow=spark.createDataFrame([(\"Cassandra\",\n",
    "df.where(col(\"_c0\") == \"Description\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Primary_database_model\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"DB-Engines_Ranking\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Trend_Chart\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Website\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Technical_documentation\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Developer\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Initial_release\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Current_release\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"License\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Cloud-based_only\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Implementation_language\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Server_operating_systems\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Data_scheme\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Typing\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"XML_support\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Secondary_indexes\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"SQL\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"APIs_and_other_access_methods\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Supported_programming_languages\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Server-side_scripts\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Triggers\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Partitioning_methods\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Replication_methods\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"MapReduce\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Consistency_concepts\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Foreign_keys\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Transaction_concepts\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Concurrency\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Durability\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"In-memory_capabilities\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"User_concepts\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Specific_characteristics\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Competitive_advantages\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Typical_application_scenarios\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Key_customers\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Market_metrics\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Licensing_and_pricing_models\").select('_c2').head()[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfuk=dfInitial.union(dfRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4295.save.\n: java.io.IOException: Unable to clear output directory file:/C:/tmp/hive prior to writing to it\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:233)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:129)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:123)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:173)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:211)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:208)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:109)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:828)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:828)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:309)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:236)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-5c119c0b19ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"overwrite\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sep\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/tmp/hive/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\spark\\spark-3.0.0\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.0\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1286\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.0\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\spark\\spark-3.0.0\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o4295.save.\n: java.io.IOException: Unable to clear output directory file:/C:/tmp/hive prior to writing to it\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:233)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:129)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:123)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:173)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:211)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:208)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:109)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:828)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$4(SQLExecution.scala:100)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:87)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:828)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:309)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:236)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "dfuk.write.format(\"csv\")\\\n",
    ".mode(\"overwrite\")\\\n",
    ".option(\"sep\", \"\\t\")\\\n",
    ".save(\"C:/tmp/hive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+--------------------+------+--------------------+\n",
      "|                 _c0| _c1|                 _c2|   _c3|                 _c4|\n",
      "+--------------------+----+--------------------+------+--------------------+\n",
      "|                Name|null|           Cassandra|  null|                null|\n",
      "|         Description|null|Wide-column store...|  null|                null|\n",
      "|Primary_database_...|null|   Wide column store|  null|                null|\n",
      "| DB-Engines_Ranking |null|               Score|121.09|                null|\n",
      "|         Trend_Chart|null|                Rank|   #10|             Overall|\n",
      "|                null|null|                null|    #1|  Wide column stores|\n",
      "|             Website|null|cassandra.apache.org|  null|                null|\n",
      "|Technical_documen...|null|cassandra.apache....|  null|                null|\n",
      "|           Developer|null|Apache Software F...|  null|                null|\n",
      "|     Initial_release|null|                2008|  null|                null|\n",
      "|     Current_release|null|3.11.6, February ...|  null|                null|\n",
      "|            License |null|        Open Source |  null|                null|\n",
      "|   Cloud-based_only |null|                  no|  null|                null|\n",
      "|DBaaS_offerings (...|null|                null|  null|                null|\n",
      "|Implementation_la...|null|                Java|  null|                null|\n",
      "|Server_operating_...|null|                 BSD|  null|                null|\n",
      "|                null|null|               Linux|  null|                null|\n",
      "|                null|null|                OS X|  null|                null|\n",
      "|                null|null|             Windows|  null|                null|\n",
      "|         Data_scheme|null|         schema-free|  null|                null|\n",
      "|             Typing |null|                 yes|  null|                null|\n",
      "|        XML_support |null|                  no|  null|                null|\n",
      "|   Secondary_indexes|null|         restricted |  null|                null|\n",
      "|                SQL |null|SQL-like SELECT, ...|  null|                null|\n",
      "|APIs_and_other_ac...|null|Proprietary proto...|  null|                null|\n",
      "|                null|null|              Thrift|  null|                null|\n",
      "|Supported_program...|null|                  C#|  null|                null|\n",
      "|                null|null|                 C++|  null|                null|\n",
      "|                null|null|             Clojure|  null|                null|\n",
      "|                null|null|              Erlang|  null|                null|\n",
      "|                null|null|                  Go|  null|                null|\n",
      "|                null|null|             Haskell|  null|                null|\n",
      "|                null|null|                Java|  null|                null|\n",
      "|                null|null|         JavaScript |  null|                null|\n",
      "|                null|null|                null|  null|                null|\n",
      "|                null|null|                Perl|  null|                null|\n",
      "|                null|null|                 PHP|  null|                null|\n",
      "|                null|null|              Python|  null|                null|\n",
      "|                null|null|                Ruby|  null|                null|\n",
      "|                null|null|               Scala|  null|                null|\n",
      "|Server-side_scripts |null|                  no|  null|                null|\n",
      "|            Triggers|null|                 yes|  null|                null|\n",
      "|Partitioning_meth...|null|           Sharding |  null|                null|\n",
      "|Replication_methods |null|selectable replic...|  null|                null|\n",
      "|          MapReduce |null|                 yes|  null|                null|\n",
      "|Consistency_conce...|null|Eventual Consistency|  null|                null|\n",
      "|                null|null|Immediate Consist...|  null|                null|\n",
      "|                null|null|                null|  null|                null|\n",
      "|       Foreign_keys |null|                  no|  null|                null|\n",
      "|Transaction_conce...|null|                 no |  null|                null|\n",
      "|        Concurrency |null|                 yes|  null|                null|\n",
      "|         Durability |null|                 yes|  null|                null|\n",
      "|In-memory_capabil...|null|                  no|  null|                null|\n",
      "|      User_concepts |null|Access rights for...|  null|                null|\n",
      "|Specific_characte...|null|Apache Cassandra ...|  null|                null|\n",
      "|Competitive_advan...|null|No single point o...|  null|                null|\n",
      "|                null|null|Operational simpl...|  null|                null|\n",
      "|                null|null|Best-in-class sca...|  null|                null|\n",
      "|                null|null|                null|  null|                null|\n",
      "|Typical_applicati...|null|Internet of Thing...|  null|                null|\n",
      "|       Key_customers|null|Apple, Netflix,  ...|  null|                null|\n",
      "|      Market_metrics|null|Cassandra is used...|  null|                null|\n",
      "|Licensing_and_pri...|null|     Apache license |  null|                null|\n",
      "|                null|null|                null|  null|                null|\n",
      "|                null|null|Pricing for comme...|  null|                null|\n",
      "+--------------------+----+--------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.format(\"csv\").option(\"inferSchema\", \"true\").load(\"Cassandra.csv\")\n",
    "df.count()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "df.show(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+----+----+\n",
      "| _c0| _c1|      _c2| _c3| _c4|\n",
      "+----+----+---------+----+----+\n",
      "|Name|null|Cassandra|null|null|\n",
      "+----+----+---------+----+----+\n",
      "\n",
      "+----+----+---------+----+----+\n",
      "| _c0| _c1|      _c2| _c3| _c4|\n",
      "+----+----+---------+----+----+\n",
      "|Name|null|Cassandra|null|null|\n",
      "+----+----+---------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col, column \n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "df.where(col(\"_c0\") == \"Name\").show()\n",
    "spark.sql(\"select * from dfTable where _c0 ='Name' limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = StructType([\n",
    "StructField(\"Description\", StringType(), True),\n",
    "StructField(\"Primary_database_model\", StringType(), True),\n",
    "StructField(\"DB-Engines_Ranking \", StringType(), True),\n",
    "StructField(\"Trend_Chart\", StringType(), True),\n",
    "StructField(\"Website\", StringType(), True),\n",
    "StructField(\"Technical_documentation\", StringType(), True),\n",
    "StructField(\"Developer\", StringType(), True),\n",
    "StructField(\"Initial_release\", StringType(), True),\n",
    "StructField(\"Current_release\", StringType(), True),\n",
    "StructField(\"License\", StringType(), True),\n",
    "StructField(\"Cloud-based_only\", StringType(), True),\n",
    "StructField(\"Implementation_language\", StringType(), True),\n",
    "StructField(\"Server_operating_systems\", StringType(), True),\n",
    "StructField(\"Data_scheme\", StringType(), True),\n",
    "StructField(\"Typing\", StringType(), True),\n",
    "StructField(\"XML_support\", StringType(), True),\n",
    "StructField(\"Secondary_indexes\", StringType(), True),\n",
    "StructField(\"SQL\", StringType(), True),\n",
    "StructField(\"APIs_and_other_access_methods\", StringType(), True),\n",
    "StructField(\"Supported_programming_languages\", StringType(), True),\n",
    "StructField(\"Server-side_scripts \", StringType(), True),\n",
    "StructField(\"Triggers\", StringType(), True),\n",
    "StructField(\"Partitioning_methods\", StringType(), True),\n",
    "StructField(\"Replication_methods\", StringType(), True),\n",
    "StructField(\"MapReduce\", StringType(), True),\n",
    "StructField(\"Consistency_concepts\", StringType(), True),\n",
    "StructField(\"Foreign_keys\", StringType(), True),\n",
    "StructField(\"Transaction_concepts\", StringType(), True),\n",
    "StructField(\"Concurrency\", StringType(), True),\n",
    "StructField(\"Durability\", StringType(), True),\n",
    "StructField(\"In-memory_capabilities \", StringType(), True),\n",
    "StructField(\"User_concepts\", StringType(), True),\n",
    "StructField(\"Specific_characteristics\", StringType(), True),\n",
    "StructField(\"Competitive_advantages\", StringType(), True),\n",
    "StructField(\"Typical_application_scenarios\", StringType(), True),\n",
    "StructField(\"Key_customers\", StringType(), True),\n",
    "StructField(\"Market_metrics\", StringType(), True),\n",
    "StructField(\"Licensing_and_pricing_models\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = spark.createDataFrame(spark.sparkContext.emptyRDD(), mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| _1| _2| _3|\n",
      "+---+---+---+\n",
      "| 15|Alk|Dhl|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(df.where(col(\"_c0\") == \"Name\").select('_c2').collect()))\n",
    "print(type(df.where(col(\"_c0\") == \"Name\").select('_c2').collect()[0]))\n",
    "print(type(df.where(col(\"_c0\") == \"Name\").select('_c2').head()))\n",
    "print(type(df.where(col(\"_c0\") == \"Name\").select('_c2').collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                  _1|       _2|\n",
      "+--------------------+---------+\n",
      "|Wide-column store...|Cassandra|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([(df.where(col(\"_c0\") == \"Description\").select('_c2').head()[0],\n",
    "                        df.where(col(\"_c0\") == \"Name\").select('_c2').head()[0])]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cassandra\n"
     ]
    }
   ],
   "source": [
    "print(df.where(col(\"_c0\") == \"Name\").select('_c2').head()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cassandra\n"
     ]
    }
   ],
   "source": [
    "print(df.where(col(\"_c0\") == \"Name\").select('_c2').collect()[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------------+-----+----+--------------------+--------------------+\n",
      "|       _1|                  _2|               _3|   _4|  _5|                  _6|                  _7|\n",
      "+---------+--------------------+-----------------+-----+----+--------------------+--------------------+\n",
      "|Cassandra|Wide-column store...|Wide column store|Score|Rank|Wide-column store...|cassandra.apache.org|\n",
      "+---------+--------------------+-----------------+-----+----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.format(\"csv\").option(\"inferSchema\", \"true\").load(\"Cassandra.csv\")\n",
    "spark.createDataFrame([(\"Cassandra\",\n",
    "df.where(col(\"_c0\") == \"Description\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Primary_database_model\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"DB-Engines_Ranking\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Trend_Chart\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Description\").select('_c2').head()[0],\n",
    "df.where(col(\"_c0\") == \"Website\").select('_c2').head()[0])]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='DB-Engines_Ranking')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.select(\"_c0\").collect()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
