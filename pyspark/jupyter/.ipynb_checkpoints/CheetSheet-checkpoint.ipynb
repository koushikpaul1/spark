{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"001\").master(\"local\").config(\"spark.sql.warehouse.dir\", \"file:///C:/tmp/hive\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import instr,col,column, expr, coalesce, broadcast\n",
    "from pyspark.sql.functions import count, first, last,min, max, sum, avg, sumDistinct, pow, desc, countDistinct, approx_count_distinct, dense_rank, rank\n",
    "from pyspark.sql.functions import corr, round, bround, monotonically_increasing_id, mean, format_number, var_pop, stddev_pop,var_samp, stddev_samp, skewness, kurtosis, covar_pop, covar_samp\n",
    "from pyspark.sql.functions import initcap , lower, upper, lit, ltrim, rtrim, rpad, lpad, trim, regexp_replace, regexp_extract, translate, collect_set, collect_list\n",
    "from pyspark.sql.functions import current_date, current_timestamp, date_add, date_sub, datediff, to_date, months_between, dayofmonth,hour,dayofyear,month,year,weekofyear,date_format\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Complex DataTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[2] =  15\n",
      "a[0:3] =  [5, 10, 15]\n",
      "a[5:] =  [30, 'xyz', 40]\n",
      "a[:5] =  [5, 10, 15, 20, 25]\n",
      "[5, 10, 4, 20, 25, 30, 'xyz', 40]\n"
     ]
    }
   ],
   "source": [
    "a = [5,10,15,20,25,30,'xyz',40]\n",
    "print(\"a[2] = \", a[2])      # a[2] = 15\n",
    "print(\"a[0:3] = \", a[0:3])  # a[0:3] = [5, 10, 15]\n",
    "print(\"a[5:] = \", a[5:])    # a[5:] = [30, 35, 40]\n",
    "print(\"a[:5] = \", a[:5])    # a[:5] =  [5, 10, 15, 20, 25]\n",
    "#Lists are mutable, meaning, the value of elements of a list can be altered.\n",
    "a[2] = 4\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuple (Immutable List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program\n",
      "(5, 'program', (1+3j))\n"
     ]
    }
   ],
   "source": [
    "t = (5,'program', 1+3j)\n",
    "print(t[1])      #   program\n",
    "print( t[0:3])   #  (5, 'program', (1+3j))\n",
    "# t[0] = 10 # Generates error # Tuples are immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l\n",
      "iline\n"
     ]
    }
   ],
   "source": [
    "#Python Strings\n",
    "s = \"This is a string\"\n",
    "s = '''A multiline\n",
    "string'''\n",
    "print(s[4])     # l\n",
    "print( s[6:11]) #  iline\n",
    "#s[5] ='d' # Generates error # Strings are immutable in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set (set are unordered collection, indexing has no meaning. Hence, the slicing operator [] does not work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3, 4, 5}\n",
      "<class 'set'>\n"
     ]
    }
   ],
   "source": [
    "a = {5,2,3,1,1,1,1,1,1,1,4}\n",
    "print(a)       #{1, 2, 3, 4, 5}\n",
    "print(type(a)) # <class 'set'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary (unordered collection of key-value pairs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "value\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d = {1:'value','key':2}\n",
    "print(type(d))           # <class 'dict'>\n",
    "print(d[1]);             # value\n",
    "print(d['key']);         # 2\n",
    "#print(\"d[2] = \", d[2]);  # Generates error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame\n",
    "Dataframe can be created from :\n",
    "List,\n",
    "rdd,\n",
    "reading data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1000).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")\n",
    "columns = [\"language\",\"users_count\"] #list\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\")]  #list\n",
    "ddata = [{\"language\":\"Java\", \"users_count\":\"20000\"}, {\"language\":\"Python\",\"users_count\": \"100000\"}]  #list\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data) #RDD\n",
    "\n",
    "Person = Row('language', 'users_count') # Row\n",
    "person = rdd.map(lambda r: Person(*r)) #PipelinedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\spark\\spark-3.0.0\\python\\pyspark\\sql\\session.py:375: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(language='Java', users_count='20000'),\n",
       " Row(language='Python', users_count='100000')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(data).collect() # [Row(_1='Java', _2='20000'), Row(_1='Python', _2='100000')]\n",
    "spark.createDataFrame(data, ['language', 'users_count']).collect() #[Row(language='Java', users_count='20000'), Row(langu..)]\n",
    "spark.createDataFrame(data, columns).collect() #[Row(language='Java', users_count='20000'), Row(language='Python', users_..)]\n",
    "spark.createDataFrame(ddata).collect()#[Row(language='Java', users_count='20000'), Row(language='Python', users_count='100000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(language='Java', users_count='20000'),\n",
       " Row(language='Python', users_count='100000')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(rdd).collect()                  #[Row(_1='Java', _2='20000'), Row(_1='Python', _2='100000')]\n",
    "spark.createDataFrame(rdd, [\"language\",\"users_count\"]).collect()  #[Row(language='Java', users_count='20000'), Row(langu..)]\n",
    "\n",
    "spark.createDataFrame(person).collect() #[Row(language='Java', users_count='20000'), Row(language='Python', users_count='100000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(language='Java', users_count='20000'),\n",
       " Row(language='Python', users_count='100000')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.toDF().collect() #[Row(_1='Java', _2='20000'), Row(_1='Python', _2='100000')]\n",
    "rdd.toDF(columns).collect() #[Row(language='Java', users_count='20000'), Row(language='Python', users_count='100000')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read mode  \n",
    "permissive     ==>> Sets all fields to null when it encounters a corrupted record and places all corrupted records in a string column called _corrupt_record\\\n",
    "dropMalformed ==>> Drops the row that contains malformed records\\\n",
    "failFast       ==>> Fails immediately upon encountering malformed records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-9efc542558ef>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-9efc542558ef>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    spark.read.text(\"filePath/fileName.txt\").selectExpr(\"split(value, ',') as rows\").\u001b[0m\n\u001b[1;37m                                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"filePath/fileName.csv\", inferSchema = True, header = True)\n",
    "spark.read.csv(\"filePath/fileName.csv\", inferSchema = True, header = True , mode = 'FAILFAST')\n",
    "\n",
    "spark.read.csv(\"filePath/fileName.csv\")\n",
    "spark.read.text(\"filePath/fileName.txt\").selectExpr(\"split(value, ',') as rows\").\n",
    "spark.read.json(\"filePath/fileName.json\")\n",
    "spark.read.parquet(\"filePath/fileName.parquet\")\n",
    "spark.read.format(\"avro\").load(\"filePath/fileName.avro\")\n",
    "\n",
    "spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"mode\", \"FAILFAST\")\\\n",
    "\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .load(\"filePath/fileName.csv\")\n",
    "spark.read.format(\"jdbc\")\\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "        .option(\"url\", \"jdbc:postgresql://database_server\")\\\n",
    "        .option(\"dbtable\", \"schema.tablename\")\\\n",
    "        .option(\"user\", \"username\").option(\"password\", \"my-secret-password\").load()\n",
    "spark.readStream.format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", \"192.168.1.100:9092\")\\\n",
    "        .option(\"subscribe\", \"json_topic\")\\\n",
    "        .option(\"startingOffsets\", \"earliest\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save mode \n",
    "append ==>> Appends the output files to the list of files that already exist at that location\\\n",
    "overwrite ==>> Will completely overwrite any data that already exists there\\\n",
    "errorIfExists ==>> Throws an error and fails the write if data or files already exist at the specified location\\\n",
    "ignore ==>> If data or files exist at the location, do nothing with the current DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"json\").save(\"filePath/fileName.json\")        \n",
    "df.write.format(\"json\").mode(\"overwrite\").save(\"filePath/fileName.json\") \n",
    "df.write.mode('append').parquet(\"filePath/fileName.parquet\")\n",
    "df.write.mode('overwrite').parquet(\"filePath/fileName.parquet\")\n",
    "df.write.partitionBy(\"gender\",\"salary\").parquet(\"filePath/fileName.parquet\")\n",
    "df.write.partitionBy(\"dob_year\",\"dob_month\").format(\"avro\").save(\"person_partition.avro\")\n",
    "df.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\"filePath/fileName.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With explicit Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})])\n",
    "df = spark.read.format(\"json\").schema(myManualSchema).load(\"filePath/fileName.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame available methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.collect()\t\n",
    "df.coalesce(1).rdd.getNumPartitions()\t\n",
    "df.count()\t\n",
    "df.createGlobalTempView()\t\n",
    "df.distinct().count()\t\n",
    "df.drop('age').collect()\n",
    "df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
    "df.dropDuplicates().show()\t\n",
    "df.dtypes\t\n",
    "df.explain()\t\n",
    "df.filter(df.age>3).collect()\t\n",
    "df.where(df.age==2).collect()\t\n",
    "df.first()\t\n",
    "df.foreach(f)\t\n",
    "df.foreachPartition(f)\t\n",
    "df.head()\t\n",
    "df.join(df2,'name','outer').select('name','height').collect()\t\n",
    "df.limit(1).collect()\t\n",
    "df.orderBy(df.age.desc()).collect()\t\n",
    "df.persist(StorageLevel.DISK_ONLY_2).storageLevel\t\n",
    "df.printSchema()\t\n",
    "df.registerTempTable(\"people\")\t\n",
    "df.repartition(10).rdd.getNumPartitions()\t\n",
    "df.rollup(\"name\",df.age).count().orderBy(\"name\",\"age\").show()\t\n",
    "df.sample(False,0.5,42).count()\t\n",
    "df.schema\t\n",
    "df.select('*').collect()\t\n",
    "df.selectExpr(\"age*2\",\"abs(age)\").collect()\t\n",
    "df.sort(df.age.desc()).collect()\t\n",
    "df.sortWithinPartitions(\"age\",ascending=False).show()\t\n",
    "df.storageLevel,df.cache().storageLevel\t\n",
    "df.take(2)\t\n",
    "df.toDF('f1','f2').collect()\t\n",
    "df.toJSON().first()\t\n",
    "df.union(df).repartition(\"age\")\t\n",
    "df.withColumn('age2',df.age+2).collect()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.json(\"data/people.json\")\n",
    "from pyspark.sql import Row\n",
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df.columns         #['age', 'name']\n",
    "df.count()         #3\n",
    "df.describe()      #DataFrame[summary: string, age: string, name: string]\n",
    "df.select('age')   #  DataFrame[age: bigint]\n",
    "df.first()         # returns first row \n",
    "df.collect()       #  Returns a list containing all the rows of the Dataframe [Row(Date='2010-01-04',......\n",
    "df.head(3)          #  top 3 rows in a list. Row(Date='2010-01-04', Open=213.429998, High=214\n",
    "df.take(3)         #  [Row(Date='2010-01-04', Open=213.429998, High=214\n",
    "#df.head() returns the top Row   #  df.take(1) returns a list of one Row(top)\n",
    "#df.head(1)=df.take(1) returns a list of one Row(top)\n",
    "\n",
    "df.withColumn('newage',df['age']*10).show()\n",
    "\n",
    "type(df.columns)            # List\n",
    "type(df['age'])             #pyspark.sql.column.Column\n",
    "type(df.select('age'))      #pyspark.sql.dataframe.DataFrame\n",
    "type(df.head())             #pyspark.sql.types.Row\n",
    "type(df.take(3))            # List\n",
    "type(df.collect())          # List\n",
    "\n",
    "\n",
    "myRow = Row(\"Hello\", None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('age').show()\n",
    "df.select('age','name').show()\n",
    "df.select(['age','name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter, Where "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "df = spark.read.csv('data/appl_stock.csv',inferSchema=True,header=True)\n",
    "result=df.filter((col('Close')<500) & (col('Close')>498) ).collect()   # list\n",
    "row=result[0]                                                          # pyspark.sql.types.Row\n",
    "row.asDict()['Volume']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"Close<500\").show(2)\n",
    "df.filter('Close<500').select('open','close').show(2)\n",
    "df.filter(df['close']<500).select(['open','close']).show(2)\n",
    "df.filter((df['Open'] <200) & (df['close'] > 200)).show(2)\n",
    "df.filter((col('Close')<500) & (col('Close')>200) ).show(2)\n",
    "\n",
    "df.where(\"Open < 213\").show(2)\n",
    "df.where(\"Open < 213\").where(col('High')> 215).show(2)\n",
    "df.where(col(\"Open\")< 213).where('High > 215').select('Open','Close','High','Low').show(2)\n",
    "df.where((col('Close')<500) & (col('Close')>200) ).show(2)\n",
    "\n",
    "df.where('High = 215.23').show(2)\n",
    "df.where('High != 215.23').show(2)\n",
    "df.where('High <> 215.23').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr ,expr \n",
    "df = spark.read.json('data/2015-summary.json')\n",
    "\n",
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n",
    "\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter)).where(\"isExpensive\")\\\n",
    ".select(\"unitPrice\", \"isExpensive\").show(5)\n",
    "\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\")).where(\"isExpensive\")\\\n",
    ".select(\"Description\", \"UnitPrice\").show(5)\n",
    "\n",
    "df.where(col(\"Description\").eqNullSafe(\"hello\")).show()\n",
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n",
    ".where(\"hasSimpleColor\").select(\"Description\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Col, Column, Expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, column ,lit\n",
    "df=spark.read.json('data/2015-summary.json')\n",
    "df=spark.read.format(\"json\").load('data/2015-summary.json')\n",
    "\n",
    "df.select(\n",
    "expr(\"DEST_COUNTRY_NAME\"),\n",
    "col(\"DEST_COUNTRY_NAME\"),\n",
    "column(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\")).show(2)\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "\n",
    "df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(2)\n",
    "#SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry FROM dfTable LIMIT 2.\n",
    "\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding ,Renaming, Dropping, Casting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"numberOne\", lit(1)).show(2)\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)\n",
    "df.selectExpr( \"*\" ,\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").show(2)\n",
    "\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.withColumn(\"countLong\", col(\"count\").cast(\"long\")).drop(\"count\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().show()\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()                          #125 unique Country\n",
    "\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().show()\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()     #125 unique combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### desc,    asc    ,asc_nulls_last ,    asc_nulls_first,      desc_nulls_last,     desc_nulls_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "df.sort(desc('count')).show(2)\n",
    "df.orderBy('count', ascending=False).show(2)\n",
    "df.orderBy(col(\"count\").desc()).show(2)\n",
    "\n",
    "\n",
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)\n",
    "df.orderBy(col(\"count\").desc_nulls_first(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repartition and Coalesce\n",
    "If you know that you’re going to be filtering by a certain column often, it can be worth repartitioning based on that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()\n",
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\n",
    "df.repartition(col(\"DEST_COUNTRY_NAME\")).rdd.getNumPartitions()\n",
    "#This operation will shuffle your data into five partitions based on the destination country name, and then coalesce them (without a full shuffle):\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting Rows to the Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectDF = df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count\n",
    "collectDF.show() # this prints it out nicely\n",
    "collectDF.show(5, False)\n",
    "collectDF.show(5, True)\n",
    "collectDF.collect()\n",
    "\n",
    "# when this is called  data from all the partitions doesnt go to driver at the ssame time, but one after another .\n",
    "collectDF.toLocalIterator() #generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structs\n",
    "    DataFrames within DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"data/2010-12-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\").show(2)\n",
    "df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\").show(2)\n",
    "from pyspark.sql.functions import struct\n",
    "df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split,col\n",
    "df.select(split(\"Description\", \" \")).show(2)\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Contains   : instr also does the same thing \n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    ".select(\"Description\", \"InvoiceNo\", \"splitted\", \"exploded\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Map from aDataframe ( Key value pair ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import create_map ,split,col\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).head()\n",
    "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    ".selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF = spark.range(1).selectExpr(\"\"\"'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spark.range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_json_object ,json_tuple\n",
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "jsonDF.select(get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias('Column') ,\n",
    "json_tuple(col(\"jsonString\"), \"myJSONKey\").alias('Tuple')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_json (from Struct to Json)\n",
    "from pyspark.sql.functions import to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_json (from Json to specified schema)\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "parseSchema = StructType((\n",
    "StructField(\"InvoiceNo\",StringType(),True),\n",
    "StructField(\"Description\",StringType(),True)))\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n",
    ".select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\") \n",
    "def power3(double_value): return double_value ** 3 # Define UDF\n",
    "power3(2.0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you use the function, there are essentially two different things that occur. \n",
    "If the function is written in Scala or Java, you can use it within the Java Virtual Machine (JVM). This means that there will be little performance penalty aside from the fact that you can’t take advantage of code generation capabilities that Spark has for built-in functions. There can be performance issues if you create or use a lot of objects\n",
    "\n",
    "If the function is written in Python, something quite different happens. Spark starts a Python process on the worker, serializes all of the data to a format that Python can understand (remember, it was in the JVM earlier), executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark.\n",
    "\n",
    "\n",
    "### Thats why UDF is not a good choice in spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)                        # Register the UDF\n",
    "from pyspark.sql.functions import col\n",
    "udfExampleDF.select(power3udf(col(\"num\"))).show(2)     # Use the uDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col(\"InvoiceNo\") != 536365).select(\"InvoiceNo\", \"Description\").show(5, True)\n",
    "df.where(\"InvoiceNo = 536365\").show(2,False)\n",
    "df.where(\"InvoiceNo <> 536365\").show(2,True)\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\")).where(\"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1 # instr = contains()\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter)).where(\"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(col(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)\n",
    "df.selectExpr(\"CustomerId\",\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(round(col(\"UnitPrice\"), 1).alias(\"rounded\"), col(\"UnitPrice\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row Number / Rownum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('RowNum',monotonically_increasing_id()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(col(\"Description\"),initcap(col(\"Description\")), lower(col(\"Description\")),upper (col(\"Description\") )).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n",
    "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
    "trim(lit(\" HELLO \")).alias(\"trim\"),\n",
    "lpad(lit(\"HELLO\"), 10, \" \").alias(\"lp\"),\n",
    "rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"   \n",
    "\n",
    "# Replace anything from the StringList with \"COLOR\"\n",
    "df.select(regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),col(\"Description\")).show(2)\n",
    "\n",
    "# replace L with 1, E with 3, T with 7 \n",
    "df.select(translate(col(\"Description\"), \"LET\", \"137\"),col(\"Description\")).show(2) \n",
    "\n",
    " # pull first occurence \n",
    "extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "df.select(regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),col(\"Description\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contaics/ instr \n",
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite).select(\"Description\", \"hasSimpleColor\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF = spark.range(10).withColumn(\"today\", current_date()).withColumn(\"now\", current_timestamp())\n",
    "dateDF.show(2)\n",
    "\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7)).select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n",
    "\n",
    "\n",
    "#to_date function allows you to convert a string to a date\n",
    "dateDF.select(\n",
    "to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n",
    ".select(months_between(col(\"start\"), col(\"end\"))).show(1)\n",
    "\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/appl_stock.csv\",header=True,inferSchema=True) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(dayofmonth(df['Date'])).show(2)\n",
    "df.select(dayofmonth(col('Date')).alias('DayOfMonth')).show(2)\n",
    "\n",
    "df.select(hour(df['Date'])).show(2)\n",
    "df.withColumn(\"Year\",year(df['Date'])).show(2)\n",
    "\n",
    "newdf = df.withColumn(\"Year\",year(df['Date']))\n",
    "newdf.groupBy(\"Year\").mean()[['avg(Year)','avg(Close)']].show()\n",
    "\n",
    "newdf.groupBy(\"Year\").mean()[['avg(Year)','avg(Close)']].withColumnRenamed(\"avg(Year)\",\"Year\").select('Year',format_number('avg(Close)',2).alias(\"Mean Close\")).show()\n",
    "\n",
    "df.na.replace([\"2010-01-04\"], [\"UNKNOWN\"], \"Date\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop n Fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the missing data\n",
    "\n",
    "You can use the .na functions for missing data. The drop command has the following parameters:\n",
    "\n",
    "    df.na.drop(how='any', thresh=None, subset=None)\n",
    "    \n",
    "    * param how: 'any' or 'all'.    \n",
    "        If 'any', drop a row if it contains any nulls.\n",
    "        If 'all', drop a row only if all its values are null.    \n",
    "    * param thresh: int, default None    \n",
    "        If specified, drop rows that have less than `thresh` non-null values.\n",
    "        This overwrites the `how` parameter.        \n",
    "    * param subset: \n",
    "        optional list of column names to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/ContainsNull.csv',inferSchema=True,header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row that contains missing data\n",
    "df.na.drop().show()\n",
    "\n",
    "# Drop any row that contains 2 null values\n",
    "df.na.drop(thresh=2).show()\n",
    "\n",
    "# Drop the rows where null is present in column, which name is in the subset list.\n",
    "df.na.drop(subset=[\"Sales\"]).show()\n",
    "\n",
    "df.na.drop(how='any').show()\n",
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill the missing values\n",
    "\n",
    "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill('Null was here').show()\n",
    "\n",
    "df.na.fill(0).show()\n",
    "\n",
    "df.na.fill('No Name',subset=['Name']).show() #specify what columns \n",
    "\n",
    "df.na.fill(df.select(mean(df['Sales'])).collect()[0][0],['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregate functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"data/all/*.csv\").coalesce(5)\n",
    "\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "df.printSchema()\n",
    "df.show(2)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()\n",
    "df.select(sum(\"Quantity\")).show()\n",
    "\n",
    "df.select(countDistinct(\"StockCode\")).show()\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),covar_pop(\"InvoiceNo\", \"Quantity\")).show()\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()\n",
    "\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(2)                                        # This count is an action\n",
    "df.groupBy(\"InvoiceNo\").agg(count(\"Quantity\").alias(\"quan\"),expr(\"count(Quantity)\")).show(2) # This count is a transformation\n",
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "windowSpec = Window.partitionBy(\"CustomerId\", \"date\").orderBy(desc(\"Quantity\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)\n",
    "\n",
    "\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    ".select(\n",
    "col(\"CustomerId\"),\n",
    "col(\"date\"),\n",
    "col(\"Quantity\"),\n",
    "purchaseRank.alias(\"quantityRank\"),\n",
    "purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull = dfWithDate.na.drop(subset=[\"date\"]) #dfWithDate.drop()  # Removing nulls \n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")\n",
    "\n",
    "# Grouping set only avaiable in SQL # Normal way in SQL\n",
    "spark.sql(\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull \\\n",
    "          GROUP BY customerId, stockCode ORDER BY CustomerId DESC, stockCode DESC\").show(2)\n",
    "\n",
    "# With grouping set, it helps when complex/ multiple level grouping is needed.\n",
    "spark.sql(\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull GROUP BY customerId, stockCode \\\n",
    "          GROUPING SETS((customerId, stockCode)) ORDER BY CustomerId DESC, stockCode DESC\").show(2)\n",
    "\n",
    "# can add more sets \n",
    "spark.sql(\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull GROUP BY customerId, stockCode \\\n",
    "GROUPING SETS((customerId, stockCode),()) ORDER BY CustomerId DESC, stockCode DESC \").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rollups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\\\n",
    ".selectExpr(\"date\", \"Country\", \"`sum(Quantity)` as total_quantity\").orderBy(\"Date\")\n",
    "rolledUpDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The total across all dates and countries\n",
    "#The total for each date across all countries\n",
    "#The total for each country on each date\n",
    "#The total for each country across all dates\n",
    "from pyspark.sql.functions import sum\n",
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\"))).select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"sum(Quantity)\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person          = spark.createDataFrame([    \n",
    "    (0, \"Bill Chambers\", 0, [100]),\n",
    "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "    (2, \"Michael Armbrust\", 1, [250, 100])\n",
    "     ]).toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")\n",
    "     ]).toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "\n",
    "sparkStatus     = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")\n",
    "     ]).toDF(\"id\", \"status\")\n",
    "\n",
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")\n",
    "\n",
    "#join the graduateProgram DataFrame with the person DataFrame to create a new DataFrame:\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "\n",
    "# by default = inner, others are outer, left_outer, right_outer\n",
    "person.join(graduateProgram, joinExpression).show()\n",
    "person.join(graduateProgram, joinExpression, 'right_outer').show()\n",
    "\n",
    "# Joins on Complex Types\n",
    "\n",
    "# Person.spark_status[].contains(sparkStatus.id)\n",
    "person.withColumnRenamed(\"id\", \"personId\").join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpr = person[\"graduate_program\"] == person[\"id\"]\n",
    "person.join(broadcast(graduateProgram), joinExpr).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Pushdown\n",
    "#Reading only specific columns\n",
    "#We could filter data make them arrive in their own partitions.\n",
    "props = {\"driver\":\"org.sqlite.JDBC\"}\n",
    "predicates = [\n",
    "\"DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'\",\n",
    "\"DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'\"]\n",
    "spark.read.jdbc(url, tablename, predicates=predicates, properties=props).show()\n",
    "spark.read.jdbc(url,tablename,predicates=predicates,properties=props).rdd.getNumPartitions() # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark can read data from db directly , with a option for paralilsm and bounday like sqoop.\n",
    "colName = \"count\"\n",
    "lowerBound = 0L\n",
    "upperBound = 348113L # this is the max count in our database\n",
    "numPartitions = 10\n",
    "spark.read.jdbc(url, tablename, column=colName, properties=props,lowerBound=lowerBound, upperBound=upperBound,\n",
    "numPartitions=numPartitions).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Managing File Size\n",
    "# To prevent creating file too big or two small use this\n",
    "df.write.option(\"maxRecordsPerFile\", 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe and SQL in same query ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, sum(count)FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\"\"\")\\\n",
    ".where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_collection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \") #list\n",
    "words = spark.sparkContext.parallelize(my_collection, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
