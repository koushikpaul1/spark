{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Important Links:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. https://sparkbyexamples.com/pyspark-tutorial/\n",
    "\n",
    "b. https://www.youtube.com/watch?v=3kX5ry0RCOQ&list=PL3N9eeOlCrP7MKqbOG3WL_zSJrEmJXLPx\n",
    "\n",
    "c. https://www.sqlite.org/windowfunctions.html#:~:text=A%20window%20function%20is%20an,it%20is%20a%20window%20function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"001\").master(\"local\").config(\"spark.sql.warehouse.dir\", \"file:///C:/tmp/hive\").getOrCreate()##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import instr,col,column, expr, coalesce, broadcast\n",
    "from pyspark.sql.functions import count, first, last,min, max, sum, avg, sumDistinct,sum_distinct, pow, desc, countDistinct, approx_count_distinct, dense_rank, rank\n",
    "from pyspark.sql.functions import corr, round, bround, monotonically_increasing_id, mean, format_number, var_pop, stddev_pop,var_samp, stddev_samp, skewness, kurtosis, covar_pop, covar_samp\n",
    "from pyspark.sql.functions import initcap , lower, upper, lit, ltrim, rtrim, rpad, lpad, trim, regexp_replace, regexp_extract, translate, collect_set, collect_list\n",
    "from pyspark.sql.functions import when,current_date, current_timestamp, date_add, date_sub, datediff, to_date, months_between, dayofmonth,hour,dayofyear,month,year,weekofyear,date_format\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Python Complex DataTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [5,10,15,20,25,30,'xyz',40]\n",
    "print(\"a[2] = \", a[2])      # a[2] = 15\n",
    "print(\"a[0:3] = \", a[0:3])  # a[0:3] = [5, 10, 15]\n",
    "print(\"a[5:] = \", a[5:])    # a[5:] = [30, 35, 40]\n",
    "print(\"a[:5] = \", a[:5])    # a[:5] =  [5, 10, 15, 20, 25]\n",
    "#Lists are mutable, meaning, the value of elements of a list can be altered.\n",
    "a[2] = 4\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuple (Immutable List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (5,'program', 1+3j)\n",
    "print(t[1])      #   program\n",
    "print( t[0:3])   #  (5, 'program', (1+3j))\n",
    "# t[0] = 10 # Generates error # Tuples are immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python Strings\n",
    "s = \"This is a string\"\n",
    "s = '''A multiline\n",
    "string'''\n",
    "print(s[4])     # l\n",
    "print( s[6:11]) #  iline\n",
    "#s[5] ='d' # Generates error # Strings are immutable in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set (set are unordered collection, indexing has no meaning. Hence, the slicing operator [] does not work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {5,2,3,1,1,1,1,1,1,1,4}\n",
    "print(a)       #{1, 2, 3, 4, 5}\n",
    "print(type(a)) # <class 'set'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary (unordered collection of key-value pairs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {1:'value','key':2}\n",
    "print(type(d))           # <class 'dict'>\n",
    "print(d[1]);             # value\n",
    "print(d['key']);         # 2\n",
    "#print(\"d[2] = \", d[2]);  # Generates error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame\n",
    "Dataframe can be created from : \n",
    "(1).List, (2).rdd, (3).reading data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Column1=0), Row(Column1=1), Row(Column1=2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.range(1000).toDF(\"Column1\") \n",
    "df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")\n",
    "columns = [\"language\",\"users_count\"] #list\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\")]  #list\n",
    "ddata = [{\"language\":\"Java\", \"users_count\":\"20000\"}, {\"language\":\"Python\",\"users_count\": \"100000\"}]  #list\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data) #RDD\n",
    "\n",
    "Person = Row('language', 'users_count') # Row\n",
    "person = rdd.map(lambda r: Person(*r)) #PipelinedRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating DataFrame from List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(data).collect() # [Row(_1='Java', _2='20000'), Row(_1='Python', _2='100000')]\n",
    "spark.createDataFrame(data, ['language', 'users_count']).collect() #[Row(language='Java', users_count='20000'), Row(langu..)]\n",
    "spark.createDataFrame(data, columns).collect() #[Row(language='Java', users_count='20000'), Row(language='Python', users_..)]\n",
    "spark.createDataFrame(ddata).collect()#[Row(language='Java', users_count='20000'), Row(language='Python', users_count='100000')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating DataFrame from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.toDF().collect()\n",
    "\n",
    "spark.createDataFrame(rdd).collect()                  #[Row(_1='Java', _2='20000'), Row(_1='Python', _2='100000')]\n",
    "spark.createDataFrame(rdd, [\"language\",\"users_count\"]).collect()  #[Row(language='Java', users_count='20000'), Row(langu..)]\n",
    "\n",
    "spark.createDataFrame(person).collect() #[Row(language='Java', users_count='20000'), Row(language='Python', users_count='100000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.toDF().collect() #[Row(_1='Java', _2='20000'), Row(_1='Python', _2='100000')]\n",
    "rdd.toDF(columns).collect() #[Row(language='Java', users_count='20000'), Row(language='Python', users_count='100000')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating DataFrame from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json(\"data/people.json\")\n",
    "from pyspark.sql import Row\n",
    "df.printSchema() \n",
    "df.write.format(\"json\").save(\"data/people2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "+----+-------+------+\n",
      "| age|   name|newage|\n",
      "+----+-------+------+\n",
      "|null|Michael|  null|\n",
      "|  30|   Andy|   300|\n",
      "|  19| Justin|   190|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "type(df.head())             #pyspark.sql.types.Row\n",
    "type(df.head(3))            # List  \n",
    "#type(df.take())             #error\n",
    "type(df.take(3))            # List\n",
    "type(df.collect())          # List\n",
    "type(df.columns)            # List\n",
    "type(df['age'])             #pyspark.sql.column.Column\n",
    "type(df.select('age'))      #pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "\n",
    "df.columns         #['age', 'name']\n",
    "df.count()         #3\n",
    "df.describe()      #DataFrame[summary: string, age: string, name: string]\n",
    "df.select('age')   #  DataFrame[age: bigint]\n",
    "df.first()         # returns first row \n",
    "df.collect()       #  Returns a list containing all the rows of the Dataframe [Row(Date='2010-01-04',......\n",
    "df.head(3)          #  top 3 rows in a list. Row(Date='2010-01-04', Open=213.429998, High=214\n",
    "df.take(3)         #  [Row(Date='2010-01-04', Open=213.429998, High=214\n",
    "#df.head() returns the top Row   #  df.take(1) returns a list of one Row(top)\n",
    "#df.head(1)=df.take(1) returns a list of one Row(top)\n",
    "\n",
    "\n",
    "df.select('age').show()\n",
    "df.select('age','name').show()\n",
    "df.select(['age','name']).show()\n",
    "df.withColumn('newage',df['age']*10).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Col, Column, Expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, column ,lit\n",
    "df=spark.read.json('data/2015-summary.json')\n",
    "df=spark.read.format(\"json\").load('data/2015-summary.json')\n",
    "\n",
    "df.select(\n",
    "expr(\"DEST_COUNTRY_NAME\"),\n",
    "col(\"DEST_COUNTRY_NAME\"),\n",
    "column(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\")).show(2)\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "\n",
    "df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(2)\n",
    "#SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry FROM dfTable LIMIT 2.\n",
    "\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n",
    "\n",
    "from pyspark.sql.functions import expr, col, column ,lit\n",
    "#df=spark.read.json('data/2015-summary.json')\n",
    "#df=spark.read.format(\"json\").load('data/2015-summary.json')\n",
    "\n",
    "df.select(\"DEST_COUNTRY_NAME\").show(2)\n",
    "df.select(expr(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "df.select(col(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "df.select(column(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\")).show(2)\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "# df.select(expr = df.selectExpr (shorthand)\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(2)\n",
    "#SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry FROM dfTable LIMIT 2.\n",
    "\n",
    "#aggregation\n",
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)\n",
    "\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding ,Renaming, Dropping, Casting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"numberOne\", lit(1)).show(2)\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)\n",
    "df.selectExpr( \"*\" ,\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").show(2)\n",
    "\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.withColumn(\"countLong\", col(\"count\").cast(\"long\")).drop(\"count\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter, Where "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "df = spark.read.csv('data/appl_stock.csv',inferSchema=True,header=True)\n",
    "\n",
    "df.filter(\"Close<500\").show(2)\n",
    "df.filter('Close<500').select('open','close').show(2)\n",
    "df.filter(df['close']<500).select(['open','close']).show(2)\n",
    "df.filter((df['Open'] <200) & (df['close'] > 200)).show(2)\n",
    "df.filter((col('Close')<500) & (col('Close')>200) ).show(2)\n",
    "\n",
    "df.where(\"Open < 213\").show(2)\n",
    "df.where(\"Open < 213\").where(col('High')> 215).show(2)\n",
    "df.where(col(\"Open\")< 213).where('High > 215').select('Open','Close','High','Low').show(2)\n",
    "df.where((col('Close')<500) & (col('Close')>200) ).show(2)\n",
    "\n",
    "df.where('High = 215.23').show(2)\n",
    "df.where('High != 215.23').show(2)\n",
    "df.where('High <> 215.23').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr ,expr \n",
    "dff = spark.read.csv('data/online-retail-dataset.csv',inferSchema=True,header=True)\n",
    "\n",
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n",
    "\n",
    "dff.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter)).where(\"isExpensive\")\\\n",
    ".select(\"unitPrice\", \"isExpensive\").show(5)\n",
    "\n",
    "dff.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\")).where(\"isExpensive\")\\\n",
    ".select(\"Description\", \"UnitPrice\").show(5)\n",
    "\n",
    "dff.where(col(\"Description\").eqNullSafe(\"hello\")).show()\n",
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "\n",
    "dff.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n",
    ".where(\"hasSimpleColor\").select(\"Description\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().show()\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()                          #125 unique Country\n",
    "\n",
    "df.show()\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().show()\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()     #125 unique combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Samples & Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()\n",
    "\n",
    "dataFrames = df.randomSplit([0.25, 0.75], seed)# Splits into array of df\n",
    "dataFrames[0].count() > dataFrames[1].count() # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "schema = df.schema\n",
    "newRows = [Row(\"New Country\", \"Other Country\", 5),Row(\"New Country 2\", \"Other Country 3\", 1)]\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "\n",
    "df.union(newDF).where(\"count = 1\").where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### desc,    asc    ,asc_nulls_last ,    asc_nulls_first,      desc_nulls_last,     desc_nulls_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "df.sort(desc('count')).show(2)\n",
    "df.orderBy('count',  ascending=False).show(2)\n",
    "df.orderBy(col(\"count\").desc()).show(2)\n",
    "df.orderBy(expr(\"count desc\")).show(2)\n",
    "\n",
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)\n",
    "df.orderBy(col(\"count\").desc_nulls_first(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repartition and Coalesce\n",
    "If you know that you’re going to be filtering by a certain column often, it can be worth repartitioning based on that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()\n",
    "df.repartition(5)\n",
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\n",
    "df.repartition(col(\"DEST_COUNTRY_NAME\")).rdd.getNumPartitions()\n",
    "#This operation will shuffle your data into five partitions based on the destination country name, and then coalesce them (without a full shuffle):\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting Rows to the Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectDF = df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count\n",
    "collectDF.show() # this prints it out nicely\n",
    "collectDF.show(5, False)\n",
    "collectDF.show(5, True)\n",
    "collectDF.collect()\n",
    "\n",
    "# when this is called  data from all the partitions doesnt go to driver at the ssame time, but one after another .\n",
    "collectDF.toLocalIterator() #generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col(\"InvoiceNo\") != 536365).select(\"InvoiceNo\", \"Description\").show(5, True)\n",
    "df.where(\"InvoiceNo = 536365\").show(2,False)\n",
    "df.where(\"InvoiceNo <> 536365\").show(2,True)\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\")).where(\"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1 # instr = contains()\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter)).where(\"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(col(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)\n",
    "df.selectExpr(\"CustomerId\",\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(round(col(\"UnitPrice\"), 1).alias(\"rounded\"), col(\"UnitPrice\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row Number / Rownum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('RowNum',monotonically_increasing_id()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(col(\"Description\"),initcap(col(\"Description\")), lower(col(\"Description\")),upper (col(\"Description\") )).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n",
    "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
    "trim(lit(\" HELLO \")).alias(\"trim\"),\n",
    "lpad(lit(\"HELLO\"), 10, \" \").alias(\"lp\"),\n",
    "rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"   \n",
    "\n",
    "# Replace anything from the StringList with \"COLOR\"\n",
    "df.select(regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),col(\"Description\")).show(2)\n",
    "\n",
    "# replace L with 1, E with 3, T with 7 \n",
    "df.select(translate(col(\"Description\"), \"LET\", \"137\"),col(\"Description\")).show(2) \n",
    "\n",
    " # pull first occurence \n",
    "extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "df.select(regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),col(\"Description\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contaics/ instr \n",
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite).select(\"Description\", \"hasSimpleColor\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF = spark.range(10).withColumn(\"today\", current_date()).withColumn(\"now\", current_timestamp())\n",
    "dateDF.show(2)\n",
    "\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7)).select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n",
    "\n",
    "\n",
    "#to_date function allows you to convert a string to a date\n",
    "dateDF.select(\n",
    "to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n",
    ".select(months_between(col(\"start\"), col(\"end\"))).show(1)\n",
    "\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/appl_stock.csv\",header=True,inferSchema=True) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(dayofmonth(df['Date'])).show(2)\n",
    "df.select(dayofmonth(col('Date')).alias('DayOfMonth')).show(2)\n",
    "\n",
    "df.select(hour(df['Date'])).show(2)\n",
    "df.withColumn(\"Year\",year(df['Date'])).show(2)\n",
    "\n",
    "newdf = df.withColumn(\"Year\",year(df['Date']))\n",
    "newdf.groupBy(\"Year\").mean()[['avg(Year)','avg(Close)']].show()\n",
    "\n",
    "newdf.groupBy(\"Year\").mean()[['avg(Year)','avg(Close)']].withColumnRenamed(\"avg(Year)\",\"Year\").select('Year',format_number('avg(Close)',2).alias(\"Mean Close\")).show()\n",
    "\n",
    "df.na.replace([\"2010-01-04\"], [\"UNKNOWN\"], \"Date\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NULLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the missing data \n",
    "You can use the .na functions for missing data. The drop command has the following parameters: \n",
    "**df.na.drop(how='any', thresh=None, subset=None)**   \n",
    "* param how: 'any' or 'all'.    \n",
    "     If 'any', drop a row if it contains any nulls.\n",
    "     If 'all', drop a row only if all its values are null.    \n",
    "* param thresh: int, default None    \n",
    "     If specified, drop rows that have less than `thresh` non-null values.\n",
    "     This overwrites the `how` parameter.        \n",
    "* param subset: \n",
    "     optional list of column names to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/ContainsNull.csv',inferSchema=True,header=True)\n",
    "df.show()\n",
    "\n",
    "# Drop any row that contains missing data\n",
    "df.na.drop().show()\n",
    "\n",
    "# Drop any row that contains 2 null values\n",
    "df.na.drop(thresh=2).show()\n",
    "\n",
    "# Drop the rows where null is present in column, which name is in the subset list.\n",
    "df.na.drop(subset=[\"Sales\"]).show()\n",
    "\n",
    "df.na.drop(how='any').show()\n",
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill the missing values\n",
    "\n",
    "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill('Null was here').show()\n",
    "df.na.fill(0).show()\n",
    "df.na.fill('No Name',subset=['Name']).show() #specify what columns \n",
    "df.na.fill(df.select(mean(df['Sales'])).collect()[0][0],['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex DataTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structs** DataFrames within DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"data/2010-12-01.csv\")\n",
    "\n",
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\").show(2)\n",
    "df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\").show(2)\n",
    "\n",
    "from pyspark.sql.functions import struct\n",
    "df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array\n",
    "**Split, length, contains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split,col\n",
    "df.select(split(\"Description\", \" \")).show(2)\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)\n",
    "\n",
    "from pyspark.sql.functions import size\n",
    "df.select(size(split(col(\"Description\"), \" \"))).show(2)\n",
    "\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    ".select(\"Description\", \"InvoiceNo\", \"splitted\", \"exploded\").show(10)\n",
    "\n",
    "df.select(explode(split(\"Description\", \" \"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import create_map ,split,col\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(2)\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF = spark.range(1).selectExpr(\"\"\"'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")\n",
    "jsonDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_json_object ,json_tuple\n",
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "jsonDF.select(get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias('Column') ,\n",
    "json_tuple(col(\"jsonString\"), \"myJSONKey\").alias('Tuple')).show(2)\n",
    "\n",
    "# to_json (from Struct to Json)\n",
    "from pyspark.sql.functions import to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\"))).show(2)\n",
    "\n",
    "# from_json (from Json to specified schema)\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "parseSchema = StructType((\n",
    "StructField(\"InvoiceNo\",StringType(),True),\n",
    "StructField(\"Description\",StringType(),True)))\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n",
    ".select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\") \n",
    "def power3(double_value): return double_value ** 3 # Define UDF\n",
    "power3(2.0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When you use the function, there are essentially two different things that occur. \n",
    "If the function is written in Scala or Java, you can use it within the Java Virtual Machine (JVM). This means that there will be little performance penalty aside from the fact that you can’t take advantage of code generation capabilities that Spark has for built-in functions. There can be performance issues if you create or use a lot of objects\n",
    "If the function is written in Python, something quite different happens. Spark starts a Python process on the worker, serializes all of the data to a format that Python can understand (remember, it was in the JVM earlier), executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark.\n",
    "#### Thats why UDF is not a good choice in spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\") \n",
    "\n",
    "def power3(double_value):  # create UDF\n",
    " return double_value ** 3\n",
    "power3(2.0)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)                        # Register the UDF\n",
    "from pyspark.sql.functions import col\n",
    "udfExampleDF.select(power3udf(col(\"num\"))).show(2)     # Use the uDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"data/all/*.csv\").coalesce(5)\n",
    "\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "df.printSchema()\n",
    "df.show(2)\n",
    "df.select(count(\"*\")).show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min, max, avg, sum, mean, Variance, StandardDeviations, kewness, kurtosis, Covariance and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()\n",
    "\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()\n",
    "df.select(sum(\"Quantity\")).show()\n",
    "df.select(sum_distinct(\"Quantity\")).show()\n",
    "\n",
    "df.select(\"StockCode\").distinct().count()\n",
    "df.select(countDistinct(\"StockCode\")).show()\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()\n",
    "\n",
    "df.select(count(\"Quantity\").alias(\"total_t\"),sum(\"Quantity\").alias(\"total_p\"),avg(\"Quantity\").alias(\"avg\"),expr(\"mean(Quantity)\").alias(\"mean_p\"))\\\n",
    ".selectExpr(\"total_p/total_t\",\"avg\",\"mean_p\").show()\n",
    "\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()\n",
    "\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),covar_pop(\"InvoiceNo\", \"Quantity\")).show()\n",
    "\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(2)          # This count is an action     \n",
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().sort(desc(\"CustomerId\"),desc(\"InvoiceNo\")).show(10)   \n",
    "df.groupBy(\"InvoiceNo\").agg(count(\"Quantity\").alias(\"quan\"),expr(\"count(Quantity)\")).show(2) # This count is a transformation\n",
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person          = spark.createDataFrame([    \n",
    "    (0, \"Bill Chambers\", 0, [100]),\n",
    "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "    (2, \"Michael Armbrust\", 1, [250, 100])\n",
    "     ]).toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")\n",
    "     ]).toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "\n",
    "sparkStatus     = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")\n",
    "     ]).toDF(\"id\", \"status\")\n",
    "\n",
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")\n",
    "\n",
    "#join the graduateProgram DataFrame with the person DataFrame to create a new DataFrame:\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "\n",
    "# by default = inner, others are outer, left_outer, right_outer\n",
    "person.join(graduateProgram, joinExpression).show()\n",
    "person.join(graduateProgram, joinExpression, 'right_outer').show()\n",
    "\n",
    "# Joins on Complex Types\n",
    "\n",
    "# Person.spark_status[].contains(sparkStatus.id)\n",
    "person.withColumnRenamed(\"id\", \"personId\").join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpr = person[\"graduate_program\"] == person[\"id\"]\n",
    "person.join(broadcast(graduateProgram), joinExpr).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SparkSQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|(1 + 1)|\n",
      "+-------+\n",
      "|      2|\n",
      "+-------+\n",
      "\n",
      "+---------+-------------+-----------+\n",
      "|namespace|    tableName|isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "|         |     dfnonull|       true|\n",
      "|         |some_sql_view|       true|\n",
      "+---------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT 1 + 1\").show()\n",
    "\n",
    "spark.read.json(\"data/2015-summary.json\").createOrReplaceTempView(\"some_sql_view\") # DF => SQL\n",
    "spark.sql(\"\"\" SELECT DEST_COUNTRY_NAME, sum(count) FROM some_sql_view GROUP BY DEST_COUNTRY_NAME \"\"\").where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\").count() # SQL => DF\n",
    "\n",
    "spark.sql(\"\"\" show tables IN default \"\"\").show()\n",
    "#spark.sql(\"\"\"SELECT user_id, department, first_name FROM professors WHERE department IN (SELECT name FROM department WHERE created_date >= '2016-01-01')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+---------+-------------+-----------+\n",
      "|namespace|    tableName|isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "|  default|      flights|      false|\n",
      "|  default|  flights_csv|      false|\n",
      "|         |     dfnonull|       true|\n",
      "|         |some_sql_view|       true|\n",
      "+---------+-------------+-----------+\n",
      "\n",
      "+---------+-------------+-----------+\n",
      "|namespace|    tableName|isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "|  default|      flights|      false|\n",
      "|  default|  flights_csv|      false|\n",
      "|         |     dfnonull|       true|\n",
      "|         |some_sql_view|       true|\n",
      "+---------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" show databases \"\"\").show()\n",
    "spark.sql(\"\"\" show tables IN default \"\"\").show()\n",
    "spark.sql(\"\"\" CREATE TABLE if not exists flights (DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count LONG) USING JSON OPTIONS (path \"data/2015-summary.json\")\"\"\")\n",
    "spark.sql(\"\"\" CREATE TABLE if not exists flights_csv (DEST_COUNTRY_NAME STRING,ORIGIN_COUNTRY_NAME STRING COMMENT \"remember, the US will be most prevalent\", count LONG)USING csv OPTIONS (header true, path '/data/flight-data/csv/2015-summary.csv')\"\"\")\n",
    "spark.sql(\"\"\" show tables IN default \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Pushdown\n",
    "#Reading only specific columns\n",
    "#We could filter data make them arrive in their own partitions.\n",
    "props = {\"driver\":\"org.sqlite.JDBC\"}\n",
    "predicates = [\n",
    "\"DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'\",\n",
    "\"DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'\"]\n",
    "spark.read.jdbc(url, tablename, predicates=predicates, properties=props).show()\n",
    "spark.read.jdbc(url,tablename,predicates=predicates,properties=props).rdd.getNumPartitions() # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark can read data from db directly , with a option for paralilsm and bounday like sqoop.\n",
    "colName = \"count\"\n",
    "lowerBound = 0L\n",
    "upperBound = 348113L # this is the max count in our database\n",
    "numPartitions = 10\n",
    "spark.read.jdbc(url, tablename, column=colName, properties=props,lowerBound=lowerBound, upperBound=upperBound,\n",
    "numPartitions=numPartitions).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Managing File Size\n",
    "# To prevent creating file too big or two small use this\n",
    "df.write.option(\"maxRecordsPerFile\", 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataframe and SQL in same query .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, sum(count)FROM some_sql_view GROUP BY DEST_COUNTRY_NAME\"\"\")\\\n",
    ".where(\"DEST_COUNTRY_NAME like 'S%'\").where(\"`sum(count)` > 10\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Broadcast Variables \n",
    "Immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_collection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \") #list\n",
    "words = spark.sparkContext.parallelize(my_collection, 2)                                  # RDD\n",
    "\n",
    "supplementalData = {\"Spark\":1000, \"Definitive\":200,\"Big\":-300, \"Simple\":100}              # Dictionary / Map\n",
    "suppBroadcast = spark.sparkContext.broadcast(supplementalData)                            # Broadcast (of the map)\n",
    "suppBroadcast.value                                                                       #{'Spark':1000,'Definitive':200,'Big':-300,'Simple':100}\n",
    "words.map(lambda word: (word, suppBroadcast.value.get(word, 0))).collect()   # Print the value , if not present print 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accumulators \n",
    "Mutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = spark.read.parquet(\"data/2010-summary.parquet\")\n",
    "accChina = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def accChinaFunc(flight_row):\n",
    " destination = flight_row[\"DEST_COUNTRY_NAME\"]\n",
    " origin = flight_row[\"ORIGIN_COUNTRY_NAME\"]\n",
    " if destination == \"China\":\n",
    "  accChina.add(flight_row[\"count\"])\n",
    " if origin == \"China\":\n",
    "  accChina.add(flight_row[\"count\"])\n",
    "\n",
    "\n",
    "flights.foreach(lambda flight_row: accChinaFunc(flight_row))\n",
    "accChina.value    # 953"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.range(2, 10000000, 2)\n",
    "df2 = spark.range(2, 10000000, 4)\n",
    "step1 = df1.repartition(5)\n",
    "step12 = df2.repartition(6)\n",
    "step2 = step1.selectExpr(\"id * 5 as id\")\n",
    "step3 = step2.join(step12, [\"id\"])\n",
    "step4 = step3.selectExpr(\"sum(id)\")\n",
    "step4.collect()\n",
    "step4.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(\"data/online-retail-dataset.csv\")\\\n",
    ".repartition(2)\\\n",
    ".selectExpr(\"instr(Description, 'GLASS') >= 1 as is_glass\")\\\n",
    ".groupBy(\"is_glass\")\\\n",
    ".count()\\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read mode  \n",
    "permissive(**default**)    ==>> Sets all fields to null when it encounters a corrupted record and places all corrupted records in a string column called _corrupt_record\\\n",
    "dropMalformed ==>> Drops the row that contains malformed records\\\n",
    "failFast       ==>> Fails immediately upon encountering malformed records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(\"filePath/fileName.csv\", inferSchema = True, header = True)\n",
    "spark.read.csv(\"filePath/fileName.csv\", inferSchema = True, header = True , mode = 'FAILFAST')\n",
    "\n",
    "spark.read.csv(\"filePath/fileName.csv\")\n",
    "spark.read.text(\"filePath/fileName.txt\").selectExpr(\"split(value, ',') as rows\").\n",
    "spark.read.json(\"filePath/fileName.json\")\n",
    "spark.read.parquet(\"filePath/fileName.parquet\")\n",
    "spark.read.format(\"avro\").load(\"filePath/fileName.avro\")\n",
    "\n",
    "spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"mode\", \"FAILFAST\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"filePath/fileName.csv\")\n",
    "\n",
    "spark.read.format(\"jdbc\")\\\n",
    ".option(\"driver\", \"org.postgresql.Driver\")\\\n",
    ".option(\"url\", \"jdbc:postgresql://database_server\")\\\n",
    ".option(\"dbtable\", \"schema.tablename\")\\\n",
    ".option(\"user\", \"username\").option(\"password\", \"my-secret-password\").load()\n",
    "\n",
    "spark.readStream.format(\"kafka\")\\\n",
    ".option(\"kafka.bootstrap.servers\", \"192.168.1.100:9092\")\\\n",
    ".option(\"subscribe\", \"json_topic\")\\\n",
    ".option(\"startingOffsets\", \"earliest\").load()\n",
    "\n",
    "#With explicit Schema\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})])\n",
    "df = spark.read.format(\"json\").schema(myManualSchema).load(\"filePath/fileName.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save mode\n",
    "append ==>> Appends the output files to the list of files that already exist at that location\n",
    "overwrite ==>> Will completely overwrite any data that already exists there\n",
    "errorIfExists(**default**) ==>> Throws an error and fails the write if data or files already exist at the specified location\n",
    "ignore ==>> If data or files exist at the location, do nothing with the current DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"json\").save(\"filePath/fileName.json\")\n",
    "df.write.format(\"json\").mode(\"overwrite\").save(\"filePath/fileName.json\") \n",
    "df.write.mode('append').parquet(\"filePath/fileName.parquet\")\n",
    "df.write.mode('overwrite').parquet(\"filePath/fileName.parquet\")\n",
    "df.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\"filePath/fileName.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"gender\",\"salary\").parquet(\"filePath/fileName.parquet\")\n",
    "df.write.partitionBy(\"dob_year\",\"dob_month\").format(\"avro\").save(\"person_partition.avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame available methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect()\n",
    "df.coalesce(1).rdd.getNumPartitions()\n",
    "df.count()\n",
    "df.createGlobalTempView(\"TestTable\")\n",
    "df.distinct().count()\n",
    "df.drop('age').collect() \n",
    "df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect() \n",
    "df.dropDuplicates().show()\n",
    "df.dtypes\n",
    "df.explain()\n",
    "df.filter(df.age>3).collect()\n",
    "df.where(df.age==2).collect()\n",
    "df.first()\n",
    "df.foreach(f)\n",
    "df.foreachPartition(f)\n",
    "df.head()\n",
    "df.join(df2,'name','outer').select('name','height').collect()\n",
    "df.limit(1).collect()\n",
    "df.orderBy(df.age.desc()).collect()\n",
    "df.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
    "df.printSchema()\n",
    "df.registerTempTable(\"people\")\n",
    "df.repartition(10).rdd.getNumPartitions()\n",
    "df.rollup(\"name\",df.age).count().orderBy(\"name\",\"age\").show()\n",
    "df.sample(False,0.5,42).count()\n",
    "df.schema\n",
    "df.select('').collect()\n",
    "df.selectExpr(\"age2\",\"abs(age)\").collect()\n",
    "df.sort(df.age.desc()).collect()\n",
    "df.sortWithinPartitions(\"age\",ascending=False).show()\n",
    "df.storageLevel,df.cache().storageLevel\n",
    "df.take(2)\n",
    "df.toDF('f1','f2').collect()\n",
    "df.toJSON().first()\n",
    "df.union(df).repartition(\"age\")\n",
    "df.withColumn('age2',df.age+2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"mode\", \"FAILFAST\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"data/2010-summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Advanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"data/2010-12-01.csv\")\n",
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\").show(2)\n",
    "df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\").show(2)\n",
    "\n",
    "from pyspark.sql.functions import struct\n",
    "df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.show(2)\n",
    "windowSpec = Window.partitionBy(\"CustomerId\", \"date\").orderBy(desc(\"Quantity\")).rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    ".select(col(\"CustomerId\"),col(\"date\"),col(\"Quantity\"),purchaseRank.alias(\"quantityRank\"),purchaseDenseRank.alias(\"quantityDenseRank\"),maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull = dfWithDate.na.drop(subset=[\"date\"]) #dfWithDate.drop()  # Removing nulls \n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")\n",
    "\n",
    "# Grouping set only avaiable in SQL # Normal way in SQL\n",
    "spark.sql(\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull \\\n",
    "          GROUP BY customerId, stockCode ORDER BY CustomerId DESC, stockCode DESC\").show(2)\n",
    "\n",
    "# With grouping set, it helps when complex/ multiple level grouping is needed.\n",
    "spark.sql(\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull GROUP BY customerId, stockCode \\\n",
    "          GROUPING SETS((customerId, stockCode)) ORDER BY CustomerId DESC, stockCode DESC\").show(2)\n",
    "\n",
    "# can add more sets \n",
    "spark.sql(\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull GROUP BY customerId, stockCode \\\n",
    "GROUPING SETS((customerId, stockCode),()) ORDER BY CustomerId DESC, stockCode DESC \").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rollups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\\\n",
    ".selectExpr(\"date\", \"Country\", \"`sum(Quantity)` as total_quantity\").orderBy(\"Date\")\n",
    "rolledUpDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The total across all dates and countries\n",
    "#The total for each date across all countries\n",
    "#The total for each country on each date\n",
    "#The total for each country across all dates\n",
    "from pyspark.sql.functions import sum\n",
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\"))).select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"sum(Quantity)\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
