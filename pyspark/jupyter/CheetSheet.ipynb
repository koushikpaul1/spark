{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"001\").master(\"local\").config(\"spark.sql.warehouse.dir\", \"file:///C:/tmp/hive\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Col' from 'pyspark.sql' (E:\\spark\\spark-3.0.0\\python\\pyspark\\sql\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5222fb63e785>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoalesce\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msumDistinct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcountDistinct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapprox_count_distinct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_rank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbround\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonotonically_increasing_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_pop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev_pop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvar_samp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstddev_samp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskewness\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkurtosis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcovar_pop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcovar_samp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Col' from 'pyspark.sql' (E:\\spark\\spark-3.0.0\\python\\pyspark\\sql\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import instr,col, expr, coalesce, broadcast\n",
    "from pyspark.sql.functions import count, first, last,min, max, sum, avg, sumDistinct, pow, desc, countDistinct, approx_count_distinct, dense_rank, rank\n",
    "from pyspark.sql.functions import corr, round, bround, monotonically_increasing_id, mean, format_number, var_pop, stddev_pop,var_samp, stddev_samp, skewness, kurtosis, covar_pop, covar_samp\n",
    "from pyspark.sql.functions import initcap , lower, upper, lit, ltrim, rtrim, rpad, lpad, trim, regexp_replace, regexp_extract, translate, collect_set, collect_list\n",
    "from pyspark.sql.functions import current_date, current_timestamp, date_add, date_sub, datediff, to_date, months_between, dayofmonth,hour,dayofyear,month,year,weekofyear,date_format\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Complex DataTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [5,10,15,20,25,30,'xyz',40]\n",
    "print(\"a[2] = \", a[2])      # a[2] = 15\n",
    "print(\"a[0:3] = \", a[0:3])  # a[0:3] = [5, 10, 15]\n",
    "print(\"a[5:] = \", a[5:])    # a[5:] = [30, 35, 40]\n",
    "print(\"a[:5] = \", a[:5])    # a[:5] =  [5, 10, 15, 20, 25]\n",
    "#Lists are mutable, meaning, the value of elements of a list can be altered.\n",
    "a[2] = 4\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuple (Immutable List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (5,'program', 1+3j)\n",
    "print(t[1])      #   program\n",
    "print( t[0:3])   #  (5, 'program', (1+3j))\n",
    "t[0] = 10 # Generates error # Tuples are immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python Strings\n",
    "s = \"This is a string\"\n",
    "s = '''A multiline\n",
    "string'''\n",
    "print(s[4])     # l\n",
    "print( s[6:11]) #  iline\n",
    "s[5] ='d' # Generates error # Strings are immutable in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set (set are unordered collection, indexing has no meaning. Hence, the slicing operator [] does not work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {5,2,3,1,1,1,1,1,1,1,4}\n",
    "print(a)       #{1, 2, 3, 4, 5}\n",
    "print(type(a)) # <class 'set'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary (unordered collection of key-value pairs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {1:'value','key':2}\n",
    "print(type(d))           # <class 'dict'>\n",
    "print(d[1]);             # value\n",
    "print(d['key']);         # 2\n",
    "print(\"d[2] = \", d[2]);  # Generates error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame\n",
    "Dataframe can be created from :\n",
    "List,\n",
    "rdd,\n",
    "reading data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1000).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")\n",
    "columns = [\"language\",\"users_count\"] #list\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\")]  #list\n",
    "ddata = [{\"language\":\"Java\", \"users_count\":\"20000\"}, {\"language\":\"Python\",\"users_count\": \"100000\"}]  #list\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data) #RDD\n",
    "\n",
    "Person = Row('language', 'users_count') # Row\n",
    "person = rdd.map(lambda r: Person(*r)) #PipelinedRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(data).collect() # [Row(_1='Java', _2='20000'), Row(_1='Python', _2='100000')]\n",
    "spark.createDataFrame(data, ['language', 'users_count']).collect() #[Row(language='Java', users_count='20000'), Row(langu..)]\n",
    "spark.createDataFrame(data, columns).collect() #[Row(language='Java', users_count='20000'), Row(language='Python', users_..)]\n",
    "spark.createDataFrame(ddata).collect()#[Row(language='Java', users_count='20000'), Row(language='Python', users_count='100000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(rdd).collect()                  #[Row(_1='Java', _2='20000'), Row(_1='Python', _2='100000')]\n",
    "spark.createDataFrame(rdd, [\"language\",\"users_count\"]).collect()  #[Row(language='Java', users_count='20000'), Row(langu..)]\n",
    "\n",
    "spark.createDataFrame(person).collect() #[Row(language='Java', users_count='20000'), Row(language='Python', users_count='100000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.toDF().collect() #[Row(_1='Java', _2='20000'), Row(_1='Python', _2='100000')]\n",
    "rdd.toDF(columns).collect() #[Row(language='Java', users_count='20000'), Row(language='Python', users_count='100000')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read mode  \n",
    "permissive     ==>> Sets all fields to null when it encounters a corrupted record and places all corrupted records in a string column called _corrupt_record\\\n",
    "dropMalformed ==>> Drops the row that contains malformed records\\\n",
    "failFast       ==>> Fails immediately upon encountering malformed records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(\"filePath/fileName.csv\", inferSchema = True, header = True)\n",
    "spark.read.csv(\"filePath/fileName.csv\", inferSchema = True, header = True , mode = 'FAILFAST')\n",
    "\n",
    "spark.read.csv(\"filePath/fileName.csv\")\n",
    "spark.read.text(\"filePath/fileName.txt\")\n",
    "spark.read.json(\"filePath/fileName.json\")\n",
    "spark.read.parquet(\"filePath/fileName.parquet\")\n",
    "spark.read.format(\"avro\").load(\"filePath/fileName.avro\")\n",
    "\n",
    "spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"mode\", \"FAILFAST\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .load(\"filePath/fileName.csv\")\n",
    "spark.read.format(\"jdbc\")\\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "        .option(\"url\", \"jdbc:postgresql://database_server\")\\\n",
    "        .option(\"dbtable\", \"schema.tablename\")\\\n",
    "        .option(\"user\", \"username\").option(\"password\", \"my-secret-password\").load()\n",
    "spark.readStream.format(\"kafka\")\\\n",
    "        .option(\"kafka.bootstrap.servers\", \"192.168.1.100:9092\")\\\n",
    "        .option(\"subscribe\", \"json_topic\")\\\n",
    "        .option(\"startingOffsets\", \"earliest\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save mode \n",
    "append ==>> Appends the output files to the list of files that already exist at that location\\\n",
    "overwrite ==>> Will completely overwrite any data that already exists there\\\n",
    "errorIfExists ==>> Throws an error and fails the write if data or files already exist at the specified location\\\n",
    "ignore ==>> If data or files exist at the location, do nothing with the current DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"json\").save(\"filePath/fileName.json\")        \n",
    "df.write.format(\"json\").mode(\"overwrite\").save(\"filePath/fileName.json\") \n",
    "df.write.mode('append').parquet(\"filePath/fileName.parquet\")\n",
    "df.write.mode('overwrite').parquet(\"filePath/fileName.parquet\")\n",
    "df.write.partitionBy(\"gender\",\"salary\").parquet(\"filePath/fileName.parquet\")\n",
    "df.write.partitionBy(\"dob_year\",\"dob_month\").format(\"avro\").save(\"person_partition.avro\")\n",
    "df.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\"filePath/fileName.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With explicit Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})])\n",
    "df = spark.read.format(\"json\").schema(myManualSchema).load(\"filePath/fileName.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame available methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.collect()\t\n",
    "df.coalesce(1).rdd.getNumPartitions()\t\n",
    "df.count()\t\n",
    "df.createGlobalTempView()\t\n",
    "df.distinct().count()\t\n",
    "df.drop('age').collect()\n",
    "df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
    "df.dropDuplicates().show()\t\n",
    "df.dtypes\t\n",
    "df.explain()\t\n",
    "df.filter(df.age>3).collect()\t\n",
    "df.where(df.age==2).collect()\t\n",
    "df.first()\t\n",
    "df.foreach(f)\t\n",
    "df.foreachPartition(f)\t\n",
    "df.head()\t\n",
    "df.join(df2,'name','outer').select('name','height').collect()\t\n",
    "df.limit(1).collect()\t\n",
    "df.orderBy(df.age.desc()).collect()\t\n",
    "df.persist(StorageLevel.DISK_ONLY_2).storageLevel\t\n",
    "df.printSchema()\t\n",
    "df.registerTempTable(\"people\")\t\n",
    "df.repartition(10).rdd.getNumPartitions()\t\n",
    "df.rollup(\"name\",df.age).count().orderBy(\"name\",\"age\").show()\t\n",
    "df.sample(False,0.5,42).count()\t\n",
    "df.schema\t\n",
    "df.select('*').collect()\t\n",
    "df.selectExpr(\"age*2\",\"abs(age)\").collect()\t\n",
    "df.sort(df.age.desc()).collect()\t\n",
    "df.sortWithinPartitions(\"age\",ascending=False).show()\t\n",
    "df.storageLevel,df.cache().storageLevel\t\n",
    "df.take(2)\t\n",
    "df.toDF('f1','f2').collect()\t\n",
    "df.toJSON().first()\t\n",
    "df.union(df).repartition(\"age\")\t\n",
    "df.withColumn('age2',df.age+2).collect()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json(\"data/people.json\")\n",
    "from pyspark.sql import Row\n",
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "| age|   name|newage|\n",
      "+----+-------+------+\n",
      "|null|Michael|  null|\n",
      "|  30|   Andy|   300|\n",
      "|  19| Justin|   190|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7b1defac85f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mmyRow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Hello\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Row' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.columns         #['age', 'name']\n",
    "df.count()         #3\n",
    "df.describe()      #DataFrame[summary: string, age: string, name: string]\n",
    "df.select('age')   #  DataFrame[age: bigint]\n",
    "df.first()         # returns first row \n",
    "df.collect()       #  Returns a list containing all the rows of the Dataframe [Row(Date='2010-01-04',......\n",
    "df.head(3)          #  top 3 rows in a list. Row(Date='2010-01-04', Open=213.429998, High=214\n",
    "df.take(3)         #  [Row(Date='2010-01-04', Open=213.429998, High=214\n",
    "#df.head() returns the top Row   #  df.take(1) returns a list of one Row(top)\n",
    "#df.head(1)=df.take(1) returns a list of one Row(top)\n",
    "\n",
    "df.withColumn('newage',df['age']*10).show()\n",
    "\n",
    "type(df.columns)            # List\n",
    "type(df['age'])             #pyspark.sql.column.Column\n",
    "type(df.select('age'))      #pyspark.sql.dataframe.DataFrame\n",
    "type(df.head())             #pyspark.sql.types.Row\n",
    "type(df.take(3))            # List\n",
    "type(df.collect())          # List\n",
    "\n",
    "\n",
    "myRow = Row(\"Hello\", None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('age').show()\n",
    "df.select('age','name').show()\n",
    "df.select(['age','name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter, Where "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189093100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "df = spark.read.csv('data/appl_stock.csv',inferSchema=True,header=True)\n",
    "result=df.filter((col('Close')<500) & (col('Close')>498) ).collect()   # list\n",
    "row=result[0]                                                          # pyspark.sql.types.Row\n",
    "row.asDict()['Volume']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\"Close<500\").show(2)\n",
    "df.filter('Close<500').select('open','close').show(2)\n",
    "df.filter(df['close']<500).select(['open','close']).show(2)\n",
    "df.filter((df['Open'] <200) & (df['close'] > 200)).show(2)\n",
    "df.filter((col('Close')<500) & (col('Close')>200) ).show(2)\n",
    "\n",
    "df.where(\"Open < 213\").show(2)\n",
    "df.where(\"Open < 213\").where(col('High')> 215).show(2)\n",
    "df.where(col(\"Open\")< 213).where('High > 215').select('Open','Close','High','Low').show(2)\n",
    "df.where((col('Close')<500) & (col('Close')>200) ).show(2)\n",
    "\n",
    "df.where('High = 215.23').show(2)\n",
    "df.where('High != 215.23').show(2)\n",
    "df.where('High <> 215.23').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr ,expr \n",
    "df = spark.read.json('data/2015-summary.json')\n",
    "\n",
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n",
    "\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter)).where(\"isExpensive\")\\\n",
    ".select(\"unitPrice\", \"isExpensive\").show(5)\n",
    "\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\")).where(\"isExpensive\")\\\n",
    ".select(\"Description\", \"UnitPrice\").show(5)\n",
    "\n",
    "df.where(col(\"Description\").eqNullSafe(\"hello\")).show()\n",
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n",
    ".where(\"hasSimpleColor\").select(\"Description\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Col, Column, Expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, column ,lit\n",
    "df=spark.read.json('data/2015-summary.json')\n",
    "df=spark.read.format(\"json\").load('data/2015-summary.json')\n",
    "\n",
    "df.select(\n",
    "expr(\"DEST_COUNTRY_NAME\"),\n",
    "col(\"DEST_COUNTRY_NAME\"),\n",
    "column(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\")).show(2)\n",
    "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")).show(2)\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "\n",
    "df.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\").show(2)\n",
    "#SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry FROM dfTable LIMIT 2.\n",
    "\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding ,Renaming, Dropping, Casting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"numberOne\", lit(1)).show(2)\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)\n",
    "df.selectExpr( \"*\" ,\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").show(2)\n",
    "\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "\n",
    "df.withColumn(\"countLong\", col(\"count\").cast(\"long\")).drop(\"count\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().show()\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()                          #125 unique Country\n",
    "\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().show()\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()     #125 unique combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### desc,    asc    ,asc_nulls_last ,    asc_nulls_first,      desc_nulls_last,     desc_nulls_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "df.sort(desc('count')).show(2)\n",
    "df.orderBy('count', ascending=False).show(2)\n",
    "df.orderBy(col(\"count\").desc()).show(2)\n",
    "\n",
    "\n",
    "df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)\n",
    "df.orderBy(col(\"count\").desc_nulls_first(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repartition and Coalesce\n",
    "If you know that you’re going to be filtering by a certain column often, it can be worth repartitioning based on that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()\n",
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\n",
    "df.repartition(col(\"DEST_COUNTRY_NAME\")).rdd.getNumPartitions()\n",
    "#This operation will shuffle your data into five partitions based on the destination country name, and then coalesce them (without a full shuffle):\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting Rows to the Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectDF = df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count\n",
    "collectDF.show() # this prints it out nicely\n",
    "collectDF.show(5, False)\n",
    "collectDF.show(5, True)\n",
    "collectDF.collect()\n",
    "\n",
    "# when this is called  data from all the partitions doesnt go to driver at the ssame time, but one after another .\n",
    "collectDF.toLocalIterator() #generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structs\n",
    "    DataFrames within DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"data/2010-12-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\").show(2)\n",
    "df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\").show(2)\n",
    "from pyspark.sql.functions import struct\n",
    "df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split,col\n",
    "df.select(split(\"Description\", \" \")).show(2)\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Contains   : instr also does the same thing \n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    ".select(\"Description\", \"InvoiceNo\", \"splitted\", \"exploded\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Map from aDataframe ( Key value pair ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import create_map ,split,col\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).head()\n",
    "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    ".selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF = spark.range(1).selectExpr(\"\"\"'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spark.range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_json_object ,json_tuple\n",
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "jsonDF.select(get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias('Column') ,\n",
    "json_tuple(col(\"jsonString\"), \"myJSONKey\").alias('Tuple')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_json (from Struct to Json)\n",
    "from pyspark.sql.functions import to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_json (from Json to specified schema)\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "parseSchema = StructType((\n",
    "StructField(\"InvoiceNo\",StringType(),True),\n",
    "StructField(\"Description\",StringType(),True)))\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n",
    ".select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\") \n",
    "def power3(double_value): return double_value ** 3 # Define UDF\n",
    "power3(2.0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you use the function, there are essentially two different things that occur. \n",
    "If the function is written in Scala or Java, you can use it within the Java Virtual Machine (JVM). This means that there will be little performance penalty aside from the fact that you can’t take advantage of code generation capabilities that Spark has for built-in functions. There can be performance issues if you create or use a lot of objects\n",
    "\n",
    "If the function is written in Python, something quite different happens. Spark starts a Python process on the worker, serializes all of the data to a format that Python can understand (remember, it was in the JVM earlier), executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark.\n",
    "\n",
    "\n",
    "### Thats why UDF is not a good choice in spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)                        # Register the UDF\n",
    "from pyspark.sql.functions import col\n",
    "udfExampleDF.select(power3udf(col(\"num\"))).show(2)     # Use the uDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col(\"InvoiceNo\") != 536365).select(\"InvoiceNo\", \"Description\").show(5, True)\n",
    "df.where(\"InvoiceNo = 536365\").show(2,False)\n",
    "df.where(\"InvoiceNo <> 536365\").show(2,True)\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\")).where(\"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1 # instr = contains()\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter)).where(\"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(col(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)\n",
    "df.selectExpr(\"CustomerId\",\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(round(col(\"UnitPrice\"), 1).alias(\"rounded\"), col(\"UnitPrice\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row Number / Rownum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('RowNum',monotonically_increasing_id()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(col(\"Description\"),initcap(col(\"Description\")), lower(col(\"Description\")),upper (col(\"Description\") )).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n",
    "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
    "trim(lit(\" HELLO \")).alias(\"trim\"),\n",
    "lpad(lit(\"HELLO\"), 10, \" \").alias(\"lp\"),\n",
    "rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"   \n",
    "\n",
    "# Replace anything from the StringList with \"COLOR\"\n",
    "df.select(regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),col(\"Description\")).show(2)\n",
    "\n",
    "# replace L with 1, E with 3, T with 7 \n",
    "df.select(translate(col(\"Description\"), \"LET\", \"137\"),col(\"Description\")).show(2) \n",
    "\n",
    " # pull first occurence \n",
    "extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "df.select(regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"),col(\"Description\")).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contaics/ instr \n",
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite).select(\"Description\", \"hasSimpleColor\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF = spark.range(10).withColumn(\"today\", current_date()).withColumn(\"now\", current_timestamp())\n",
    "dateDF.show(2)\n",
    "\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7)).select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n",
    "\n",
    "\n",
    "#to_date function allows you to convert a string to a date\n",
    "dateDF.select(\n",
    "to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n",
    ".select(months_between(col(\"start\"), col(\"end\"))).show(1)\n",
    "\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/appl_stock.csv\",header=True,inferSchema=True) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(dayofmonth(df['Date'])).show(2)\n",
    "df.select(dayofmonth(col('Date')).alias('DayOfMonth')).show(2)\n",
    "\n",
    "df.select(hour(df['Date'])).show(2)\n",
    "df.withColumn(\"Year\",year(df['Date'])).show(2)\n",
    "\n",
    "newdf = df.withColumn(\"Year\",year(df['Date']))\n",
    "newdf.groupBy(\"Year\").mean()[['avg(Year)','avg(Close)']].show()\n",
    "\n",
    "newdf.groupBy(\"Year\").mean()[['avg(Year)','avg(Close)']].withColumnRenamed(\"avg(Year)\",\"Year\").select('Year',format_number('avg(Close)',2).alias(\"Mean Close\")).show()\n",
    "\n",
    "df.na.replace([\"2010-01-04\"], [\"UNKNOWN\"], \"Date\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop n Fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the missing data\n",
    "\n",
    "You can use the .na functions for missing data. The drop command has the following parameters:\n",
    "\n",
    "    df.na.drop(how='any', thresh=None, subset=None)\n",
    "    \n",
    "    * param how: 'any' or 'all'.    \n",
    "        If 'any', drop a row if it contains any nulls.\n",
    "        If 'all', drop a row only if all its values are null.    \n",
    "    * param thresh: int, default None    \n",
    "        If specified, drop rows that have less than `thresh` non-null values.\n",
    "        This overwrites the `how` parameter.        \n",
    "    * param subset: \n",
    "        optional list of column names to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('data/ContainsNull.csv',inferSchema=True,header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any row that contains missing data\n",
    "df.na.drop().show()\n",
    "\n",
    "# Drop any row that contains 2 null values\n",
    "df.na.drop(thresh=2).show()\n",
    "\n",
    "# Drop the rows where null is present in column, which name is in the subset list.\n",
    "df.na.drop(subset=[\"Sales\"]).show()\n",
    "\n",
    "df.na.drop(how='any').show()\n",
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill the missing values\n",
    "\n",
    "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill('Null was here').show()\n",
    "\n",
    "df.na.fill(0).show()\n",
    "\n",
    "df.na.fill('No Name',subset=['Name']).show() #specify what columns \n",
    "\n",
    "df.na.fill(df.select(mean(df['Sales'])).collect()[0][0],['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregate functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"data/all/*.csv\").coalesce(5)\n",
    "\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "df.printSchema()\n",
    "df.show(2)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()\n",
    "df.select(sum(\"Quantity\")).show()\n",
    "\n",
    "df.select(countDistinct(\"StockCode\")).show()\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),covar_pop(\"InvoiceNo\", \"Quantity\")).show()\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()\n",
    "\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "+---------+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536596|   6|              6|\n",
      "|   536938|  14|             14|\n",
      "+---------+----+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(2)                                        # This count is an action\n",
    "df.groupBy(\"InvoiceNo\").agg(count(\"Quantity\").alias(\"quan\"),expr(\"count(Quantity)\")).show(2) # This count is a transformation\n",
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9f6f3bb4aa86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdfWithDate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"InvoiceDate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MM/d/yyyy H:mm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdfWithDate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mwindowSpec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CustomerId\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"date\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Quantity\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrowsBetween\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munboundedPreceding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrentRow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "windowSpec = Window.partitionBy(\"CustomerId\", \"date\").orderBy(desc(\"Quantity\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)\n",
    "\n",
    "\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    ".select(\n",
    "col(\"CustomerId\"),\n",
    "col(\"date\"),\n",
    "col(\"Quantity\"),\n",
    "purchaseRank.alias(\"quantityRank\"),\n",
    "purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85039B|           48|\n",
      "+----------+---------+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+---------+-------------+\n",
      "|customerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85039B|           48|\n",
      "+----------+---------+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+---------+-------------+\n",
      "|customerId|stockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85039B|           48|\n",
      "+----------+---------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNoNull = dfWithDate.na.drop(subset=[\"date\"]) #dfWithDate.drop()  # Removing nulls \n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")\n",
    "\n",
    "# Grouping set only avaiable in SQL # Normal way in SQL\n",
    "spark.sql(\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull \\\n",
    "          GROUP BY customerId, stockCode ORDER BY CustomerId DESC, stockCode DESC\").show(2)\n",
    "\n",
    "# With grouping set, it helps when complex/ multiple level grouping is needed.\n",
    "spark.sql(\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull GROUP BY customerId, stockCode \\\n",
    "          GROUPING SETS((customerId, stockCode)) ORDER BY CustomerId DESC, stockCode DESC\").show(2)\n",
    "\n",
    "# can add more sets \n",
    "spark.sql(\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull GROUP BY customerId, stockCode \\\n",
    "GROUPING SETS((customerId, stockCode),()) ORDER BY CustomerId DESC, stockCode DESC \").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rollups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+\n",
      "|      date|  Country|total_quantity|\n",
      "+----------+---------+--------------+\n",
      "|      null|     null|       1879379|\n",
      "|2010-12-01|Australia|           107|\n",
      "|2010-12-01|     null|         26814|\n",
      "|2010-12-01|  Germany|           117|\n",
      "|2010-12-01|   Norway|          1852|\n",
      "+----------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\\\n",
    ".selectExpr(\"date\", \"Country\", \"`sum(Quantity)` as total_quantity\").orderBy(\"Date\")\n",
    "rolledUpDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------+\n",
      "|      Date|       Country|sum(Quantity)|\n",
      "+----------+--------------+-------------+\n",
      "|      null|          null|      1879379|\n",
      "|      null|United Kingdom|      1595901|\n",
      "|      null|   Netherlands|        60661|\n",
      "|2011-10-05|          null|        46161|\n",
      "|2011-11-14|          null|        45959|\n",
      "+----------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The total across all dates and countries\n",
    "#The total for each date across all countries\n",
    "#The total for each country on each date\n",
    "#The total for each country across all dates\n",
    "from pyspark.sql.functions import sum\n",
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\"))).select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"sum(Quantity)\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n",
      "\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|personId|            name|graduate_program|   spark_status| id|        status|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "|       0|   Bill Chambers|               0|          [100]|100|   Contributor|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n",
      "|       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|\n",
      "|       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|\n",
      "+--------+----------------+----------------+---------------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person          = spark.createDataFrame([    \n",
    "    (0, \"Bill Chambers\", 0, [100]),\n",
    "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "    (2, \"Michael Armbrust\", 1, [250, 100])\n",
    "     ]).toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")\n",
    "     ]).toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "\n",
    "sparkStatus     = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")\n",
    "     ]).toDF(\"id\", \"status\")\n",
    "\n",
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")\n",
    "\n",
    "#join the graduateProgram DataFrame with the person DataFrame to create a new DataFrame:\n",
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']\n",
    "\n",
    "# by default = inner, others are outer, left_outer, right_outer\n",
    "person.join(graduateProgram, joinExpression).show()\n",
    "person.join(graduateProgram, joinExpression, 'right_outer').show()\n",
    "\n",
    "# Joins on Complex Types\n",
    "\n",
    "# Person.spark_status[].contains(sparkStatus.id)\n",
    "person.withColumnRenamed(\"id\", \"personId\").join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "BroadcastNestedLoopJoin BuildRight, Inner\n",
      ":- *(1) Project [_1#2887L AS id#2895L, _2#2888 AS name#2896, _3#2889L AS graduate_program#2897L, _4#2890 AS spark_status#2898]\n",
      ":  +- *(1) Filter ((isnotnull(_3#2889L) AND isnotnull(_1#2887L)) AND (_3#2889L = _1#2887L))\n",
      ":     +- *(1) Scan ExistingRDD[_1#2887L,_2#2888,_3#2889L,_4#2890]\n",
      "+- BroadcastExchange IdentityBroadcastMode, [id=#724]\n",
      "   +- *(2) Project [_1#2903L AS id#2911L, _2#2904 AS degree#2912, _3#2905 AS department#2913, _4#2906 AS school#2914]\n",
      "      +- *(2) Scan ExistingRDD[_1#2903L,_2#2904,_3#2905,_4#2906]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinExpr = person[\"graduate_program\"] == person[\"id\"]\n",
    "person.join(broadcast(graduateProgram), joinExpr).explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
