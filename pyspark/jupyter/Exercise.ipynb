{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43744c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the average/mean https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2799933550853697/1880776780418274/2202577924924539/latest.html\n",
    "#https://stackoverflow.com/questions/58507301/word-counter-with-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa5956d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc595a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import instr,col,column, expr, coalesce, broadcast\n",
    "from pyspark.sql.functions import count, first, last, min,  sum, avg, sumDistinct,sum_distinct, pow, desc, countDistinct, approx_count_distinct, dense_rank, rank\n",
    "from pyspark.sql.functions import corr, round, bround, monotonically_increasing_id, mean, format_number, var_pop, stddev_pop,var_samp, stddev_samp, skewness, kurtosis, covar_pop, covar_samp\n",
    "from pyspark.sql.functions import initcap , lower, upper, lit, ltrim, rtrim, rpad, lpad, trim, regexp_replace, regexp_extract, translate, collect_set, collect_list\n",
    "from pyspark.sql.functions import when,current_date, current_timestamp, date_add, date_sub, datediff, to_date, months_between, dayofmonth,hour,dayofyear,month,year,weekofyear,date_format\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from functools import reduce\n",
    "from operator  import add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a0b0ee",
   "metadata": {},
   "source": [
    "### WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96f20f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "book=sc.textFile(\"data/book.txt\")\n",
    "words=book.flatMap(lambda x : x.split())\n",
    "wordsAsKey=words.map(lambda x:(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30208607",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afb16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsAsKey.reduceByKey(lambda x,y: x+y).take(5)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsAsKey.reduceByKey(add).take(5)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38870d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsAsKey.groupByKey().map(lambda x: (x[0],len(x[1]))).take(5)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f6f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsAsKey.groupByKey().mapValues(lambda x: len(list(x))).take(5)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1a2625",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsAsKey.aggregateByKey(0,lambda x,y: x+y,lambda x,y: x+y).take(5)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526fd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsAsKey.combineByKey(lambda x:x,lambda x,y: x+y,lambda x,y: x+y).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e38667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsAsKey.foldByKey(0,lambda x,y: x+y).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d205f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is just extra formating  \n",
    "wordsAsKey.reduceByKey(lambda x,y: x+y).map(lambda x:(x[1],x[0])).sortByKey(False).map(lambda x:(x[1],x[0])).take(5)\n",
    "  \n",
    "wordsAsKey.reduceByKey(lambda x,y: x+y).saveAsTextFile(\"output/wordcount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d876882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word count, print only the files that start with vowel in their individual output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "book=sc.textFile(\"data/book.txt\")\n",
    "words=book.flatMap(lambda x : x.split())\n",
    "\n",
    "def myPartitioner(word):\n",
    "   if word[0].lower()=='a':return 0\n",
    "   elif word[0].lower()=='e':return 1\n",
    "   elif word[0].lower()=='i':return 2\n",
    "   elif word[0].lower()=='o':return 3\n",
    "   elif word[0].lower()=='u':return 4\n",
    "   else:  return 5\n",
    "    \n",
    "words.filter(lambda x: x[0].lower()in('a','e','i','o','u')).map(lambda x:(x,1))\\\n",
    "                                .reduceByKey(lambda x,y: x+y).partitionBy(6,myPartitioner).saveAsTextFile(\"output/wordcountP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb18f513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36325d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
