{
 "cells": [
  {
   "cell_type": "raw",
   "id": "91d97d5f",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/kimjihoo/ds4c-what-is-this-dataset-detailed-description/notebook\n",
    "The Lists of Data Table\n",
    "1) Case Data : Case: Data of COVID-19 infection cases in South Korea\n",
    "2) Patient Data\n",
    "    PatientInfo: Epidemiological data of COVID-19 patients in South Korea\n",
    "    PatientRoute: Route data of COVID-19 patients in South Korea (currently unavailable)\n",
    "3) Time Series Data\n",
    "    Time: Time series data of COVID-19 status in South Korea\n",
    "    TimeAge: Time series data of COVID-19 status in terms of the age in South Korea\n",
    "    TimeGender: Time series data of COVID-19 status in terms of gender in South Korea\n",
    "    TimeProvince: Time series data of COVID-19 status in terms of the Province in South Korea\n",
    "4) Additional Data\n",
    "    Region: Location and statistical data of the regions in South Korea\n",
    "    Weather: Data of the weather in the regions of South Korea\n",
    "    SearchTrend: Trend data of the keywords searched in NAVER which is one of the largest portals in South Korea\n",
    "    SeoulFloating: Data of floating population in Seoul, South Korea (from SK Telecom Big Data Hub)\n",
    "    Policy: Data of the government policy for COVID-19 in South Korea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a140f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7aea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import instr,col,column, expr, coalesce, broadcast,lag\n",
    "from pyspark.sql.functions import count, first, last,min, max, sum, avg, sumDistinct,sum_distinct, pow, desc, countDistinct, approx_count_distinct, dense_rank, rank\n",
    "from pyspark.sql.functions import corr, round, bround, monotonically_increasing_id, mean, format_number, var_pop, stddev_pop,var_samp, stddev_samp, skewness, kurtosis, covar_pop, covar_samp\n",
    "from pyspark.sql.functions import initcap , lower, upper, lit, ltrim, rtrim, rpad, lpad, trim, regexp_replace, regexp_extract, translate, collect_set, collect_list\n",
    "from pyspark.sql.functions import current_date, current_timestamp, date_add, date_sub, datediff, to_date, months_between, dayofmonth,hour,dayofyear,month,year,weekofyear,date_format\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType,StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce661415",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases=spark.read.csv(\"data/Case.csv\",   sep=\",\",  inferSchema=\"true\",  header=\"true\")\n",
    "cases.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases.sort('confirmed',  ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename all columns without changing columns order\n",
    "cases = cases.toDF(*['case_id', 'province', 'city', 'group', 'infection_case', 'confirmed','latitude', 'longitude'])\n",
    "cases.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change column data type\n",
    "cases.withColumn('confirmed',col('confirmed').cast(IntegerType())).show(5)\n",
    "cases.withColumn('confirmed',cases.confirmed.cast(IntegerType())).show(5)\n",
    "\n",
    "cases.select(col(\"confirmed\").cast('int').alias(\"confirmed\")).show(5)\n",
    "cases.selectExpr(\"cast(confirmed as int) confirmed\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51388ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases.groupBy([\"province\",\"city\"]).agg(sum(\"confirmed\") ,max(\"confirmed\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = spark.read.load(\"./data/Region.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\",header=\"true\")\n",
    "regions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb364dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join\n",
    "cases.join(regions, ['province','city'],how='left').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#window\n",
    "windowSpec = Window().partitionBy(['province']).orderBy(desc('confirmed'))\n",
    "cases.withColumn(\"rank\",rank().over(windowSpec)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f74ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeprovince = spark.read.load(\"data/TimeProvince.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "timeprovince.show(5)\n",
    "\n",
    "windowSpec = Window().partitionBy(['province']).orderBy('date')\n",
    "timeprovinceWithLag = timeprovince.withColumn(\"lag_7\",lag(\"confirmed\", 7).over(windowSpec))\n",
    "timeprovinceWithLag.filter(timeprovinceWithLag.date>'2020-03-10').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f60413",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window().partitionBy(['province']).orderBy('date').rowsBetween(-6,0)\n",
    "timeprovinceWithRoll = timeprovince.withColumn(\"roll_7_confirmed\",mean(\"confirmed\").over(windowSpec))\n",
    "timeprovinceWithRoll.filter(timeprovinceWithLag.date>'2020-03-10').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window().partitionBy(['province']).orderBy('date').rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "timeprovinceWithRoll = timeprovince.withColumn(\"cumulative_confirmed\",sum(\"confirmed\").over(windowSpec))\n",
    "timeprovinceWithRoll.filter(timeprovinceWithLag.date>'2020-03-10').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e35818",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotedTimeprovince = timeprovince.groupBy('date').pivot('province') \\\n",
    ".agg(sum('confirmed').alias('confirmed') , sum('released').alias('released'))\n",
    "pivotedTimeprovince.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cases.glom().map(len).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
