{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created with name as 'spark'\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "print(\"SparkSession created with name as 'spark'\")\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../../data/jose_portilla/Spark_DataFrames/ContainsNull.csv',inferSchema=True,header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop n Fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the missing data\n",
    "\n",
    "You can use the .na functions for missing data. The drop command has the following parameters:\n",
    "\n",
    "    df.na.drop(how='any', thresh=None, subset=None)\n",
    "    \n",
    "    * param how: 'any' or 'all'.    \n",
    "        If 'any', drop a row if it contains any nulls.\n",
    "        If 'all', drop a row only if all its values are null.    \n",
    "    * param thresh: int, default None    \n",
    "        If specified, drop rows that have less than `thresh` non-null values.\n",
    "        This overwrites the `how` parameter.        \n",
    "    * param subset: \n",
    "        optional list of column names to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop any row that contains missing data\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Has to have at least 2 NON-null values\n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=[\"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the missing values\n",
    "\n",
    "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+-----+\n",
      "|  Id|         Name|Sales|\n",
      "+----+-------------+-----+\n",
      "|emp1|         John| null|\n",
      "|emp2|Null was here| null|\n",
      "|emp3|Null was here|345.0|\n",
      "|emp4|        Cindy|456.0|\n",
      "+----+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('Null was here').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| null|  0.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| null|\n",
      "|emp2|No Name| null|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('No Name',subset=['Name']).show() #specify what columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.na.fill(df.select(mean(df['Sales'])).collect()[0][0],['Sales']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"../../data/jose_portilla/Spark_DataFrames/appl_stock.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|    210.58|119282800|          27.28265|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,format_number,dayofmonth,hour,dayofyear,month,year,weekofyear,date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|dayofmonth(Date)|\n",
      "+----------------+\n",
      "|               4|\n",
      "|               5|\n",
      "+----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+\n",
      "|DayOfMonth|\n",
      "+----------+\n",
      "|         4|\n",
      "|         5|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(dayofmonth(df['Date'])).show(2)\n",
    "df.select(dayofmonth(col('Date')).alias('DayOfMonth')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|hour(Date)|\n",
      "+----------+\n",
      "|         0|\n",
      "|         0|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(hour(df['Date'])).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "|      Date|      Open|      High|               Low|     Close|   Volume|         Adj Close|Year|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|214.009998|123432400|         27.727039|2010|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|214.379993|150476200|27.774976000000002|2010|\n",
      "+----------+----------+----------+------------------+----------+---------+------------------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Year\",year(df['Date'])).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|avg(Year)|        avg(Close)|\n",
      "+---------+------------------+\n",
      "|   2010.0| 259.8424600000002|\n",
      "|   2011.0|364.00432532142867|\n",
      "|   2012.0| 576.0497195640002|\n",
      "|   2013.0| 472.6348802857143|\n",
      "|   2014.0| 295.4023416507935|\n",
      "|   2015.0|120.03999980555547|\n",
      "|   2016.0|104.60400786904763|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf = df.withColumn(\"Year\",year(df['Date']))\n",
    "newdf.groupBy(\"Year\").mean()[['avg(Year)','avg(Close)']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|  Year|Mean Close|\n",
      "+------+----------+\n",
      "|2010.0|    259.84|\n",
      "|2011.0|    364.00|\n",
      "|2012.0|    576.05|\n",
      "|2013.0|    472.63|\n",
      "|2014.0|    295.40|\n",
      "|2015.0|    120.04|\n",
      "|2016.0|    104.60|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = newdf.groupBy(\"Year\").mean()[['avg(Year)','avg(Close)']]\n",
    "result = result.withColumnRenamed(\"avg(Year)\",\"Year\")\n",
    "result = result.select('Year',format_number('avg(Close)',2).alias(\"Mean Close\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.replace([\"\"], [\"UNKNOWN\"], \"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
