{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created with name as 'spark'\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"001\")\\\n",
    ".master(\"local\").config(\"spark.sql.warehouse.dir\", \"file:///C:/tmp/hive\")\\\n",
    ".getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "print(\"SparkSession created with name as 'spark'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"../../../data/SparkTheDefinitiveGuide/retail-data/by-day/2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrames within DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|             complex|InvoiceNo|StockCode|         Description|Quantity|     InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+--------------------+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|[WHITE HANGING HE...|   536365|   85123A|WHITE HANGING HEA...|       6|01-12-2010 08:26|     2.55|     17850|United Kingdom|\n",
      "|          [, 536365]|   536365|    71053|                null|       6|01-12-2010 08:26|     3.39|     17850|United Kingdom|\n",
      "+--------------------+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|             complex|InvoiceNo|StockCode|         Description|Quantity|     InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+--------------------+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|[WHITE HANGING HE...|   536365|   85123A|WHITE HANGING HEA...|       6|01-12-2010 08:26|     2.55|     17850|United Kingdom|\n",
      "|          [, 536365]|   536365|    71053|                null|       6|01-12-2010 08:26|     3.39|     17850|United Kingdom|\n",
      "+--------------------+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+\n",
      "|             complex|\n",
      "+--------------------+\n",
      "|[WHITE HANGING HE...|\n",
      "|          [, 536365]|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\").show(2)\n",
    "df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\").show(2)\n",
    "from pyspark.sql.functions import struct\n",
    "df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|split(Description,  , -1)|\n",
      "+-------------------------+\n",
      "|     [WHITE, HANGING, ...|\n",
      "|     [KNITTED, UNION, ...|\n",
      "+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split,col\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|                                            true|\n",
      "|                                           false|\n",
      "+------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Contains   : instr also does the same thing \n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------+\n",
      "|         Description|InvoiceNo|            splitted|exploded|\n",
      "+--------------------+---------+--------------------+--------+\n",
      "|WHITE HANGING HEA...|   536365|[WHITE, HANGING, ...|   WHITE|\n",
      "|WHITE HANGING HEA...|   536365|[WHITE, HANGING, ...| HANGING|\n",
      "+--------------------+---------+--------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    ".select(\"Description\", \"InvoiceNo\", \"splitted\", \"exploded\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         complex_map|\n",
      "+--------------------+\n",
      "|[WHITE HANGING HE...|\n",
      "|[KNITTED UNION FL...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access value with key \n",
    "from pyspark.sql.functions import split,col\n",
    "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    ".selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    ".selectExpr(\"explode(complex_map)\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF = spark.range(1).selectExpr(\"\"\"'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|Column|               Tuple|\n",
      "+------+--------------------+\n",
      "|     2|{\"myJSONValue\":[1...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get_json_object ,json_tuple\n",
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "jsonDF.select(get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias('Column') ,\n",
    "json_tuple(col(\"jsonString\"), \"myJSONKey\").alias('Tuple')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[myStruct: struct<InvoiceNo:string,Description:string>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to_json (from Struct to Json)\n",
    "from pyspark.sql.functions import to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|  from_json(newJSON)|             newJSON|\n",
      "+--------------------+--------------------+\n",
      "|[536365, WHITE HA...|{\"InvoiceNo\":\"536...|\n",
      "|[536365, KNITTED ...|{\"InvoiceNo\":\"536...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from_json (from Json to specified schema)\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "parseSchema = StructType((\n",
    "StructField(\"InvoiceNo\",StringType(),True),\n",
    "StructField(\"Description\",StringType(),True)))\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n",
    ".select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\") \n",
    "def power3(double_value): return double_value ** 3 # Define UDF\n",
    "power3(2.0)                                                 # Use UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you use the function, there are essentially two different things that occur. \n",
    "If the function is written in Scala or Java, you can use it within the Java Virtual Machine (JVM). This means that there will be little performance penalty aside from the fact that you canâ€™t take advantage of code generation capabilities that Spark has for built-in functions. There can be performance issues if you create or use a lot of objects\n",
    "\n",
    "If the function is written in Python, something quite different happens. Spark starts a Python process on the worker, serializes all of the data to a format that Python can understand (remember, it was in the JVM earlier), executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark.\n",
    "\n",
    "\n",
    "### Thats why UDF is not a good choice in spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)                        # Register the UDF\n",
    "from pyspark.sql.functions import col\n",
    "udfExampleDF.select(power3udf(col(\"num\"))).show(2)     # Use the uDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
