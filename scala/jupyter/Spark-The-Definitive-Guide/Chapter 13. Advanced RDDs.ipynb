{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")//String array \n",
    "val words = spark.sparkContext.parallelize(myCollection, 2)//To create an RDD from a collection, you will need to use the \n",
    "                                                 // parallelize method on a SparkContext (within a SparkSession)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyvalue=words.map(word => (word.toLowerCase, 1)) // from RDD to key-value RDD\n",
    "//keyvalue = Array((spark,1), (the,1), (definitive,1), (guide,1), (:,1), (big,1), (data,1), (processing,1),..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keyBy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyword = words.keyBy(word => word.toLowerCase.toSeq(0).toString)// a different way for key-value rdd\n",
    "//keyword = Array((s,Spark), (t,The), (d,Definitive), (g,Guide), (:,:), (b,Big), (d,Data), (p,Processing),...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyMap=keyword.mapValues(word => word.toUpperCase).collect()\n",
    "//(s,SPARK) (t,THE) (d,DEFINITIVE) (g,GUIDE) (:,:) (b,BIG) (d,DATA) (p,PROCESSING) ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyFlatMap=keyword.flatMapValues(word => word.toUpperCase).collect()\n",
    "//(s,S) (s,P) (s,A) (s,R) (s,K) (t,T) (t,H) (t,E) (d,D) (d,E) (d,F) (d,I) (d,N) (d,I) (d,T) (d,I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.keys.collect() //Array(s, t, d, g, :, b, d, p, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.values.collect()//Array(Spark, The, Definitive, Guide, :, Big, Data, Processing, Made, Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword.lookup(\"s\")//WrappedArray(Spark, Simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val distinctChars = words.flatMap(word => word.toLowerCase.toSeq).distinct.collect()//Array(d, p, t, b, h, n, f, v, :, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.map(word => (word.toLowerCase.toSeq(0), word)).sampleByKey(true, sampleMap, 6L).collect()\n",
    "//Array((s,Spark), (t,The), (d,Definitive), (g,Guide), (:,:))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val chars = words.flatMap(word => word.toLowerCase.toSeq)//(s, p, a, r, k, t, h, e, d, e, f,....\n",
    "val KVcharacters = chars.map(letter => (letter, 1))//(s,1), (p,1), (a,1), (r,1), (k,1), (t,1), (h,1), (e,1), ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### countByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val timeout = 1000L //milliseconds\n",
    "val confidence = 0.95\n",
    "KVcharacters.countByKey()//Map(e -> 7, s -> 4, n -> 2, t -> 3, u -> 1, f -> 1, a -> 4, m -> 2, i -> 7, v ..\n",
    "KVcharacters.countByKeyApprox(timeout, confidence)//(final: Map(e -> [7.000, 7.000], s -> [4.000, 4.000], n -> [2.000, 2.000].."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFunc(left:Int, right:Int) = left + right\n",
    "KVcharacters.groupByKey().map(row => (row._1, row._2.reduce(addFunc))).collect()//Array((d,4), (p,3), (t,3), (b,1)..\n",
    "KVcharacters.reduceByKey(addFunc).collect() //Array((d,4), (p,3), (t,3), (b,1),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aggregate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//aggregate function requires a null and start value and then requires you to specify two different functions. The first\n",
    "//aggregate function(max in this case) executes within partitions, the second aggregate function(add in this case) executes\n",
    "//on the first's result  in driver. (this second may cause outOofMemory).\n",
    "val nums = sc.parallelize(1 to 30, 1)// Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16\n",
    "def maxFunc(left:Int, right:Int) = math.max(left, right)\n",
    "def addFunc(left:Int, right:Int) = left + right\n",
    "println(nums.aggregate(0)(maxFunc, addFunc))// if there is only one partition the second will have only one value to add\n",
    "  //30                                           // so the result will be the mx value only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### treeAggregate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//treeAggregate  does the same thing as aggregate (at the user level) but does so in a different way. It basically “pushes down” \n",
    "//some of the subaggregations (creating a tree from executor to executor) before performing the final aggregation on the driver. \n",
    "//Having multiple levels can help you to ensure that the driver does not run out of memory in the process of the aggregation.\n",
    "//These tree-based implementations are often to try to improve stability in certain operations:\n",
    "val depth = 3\n",
    "nums.treeAggregate(0)(maxFunc, addFunc, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aggregateByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVcharacters.aggregateByKey(0)(addFunc, maxFunc).collect()// same as above for each key\n",
    "//Array((d,2), (p,2), (t,2), (b,1), (h,1), (n,1), (f,1), (v,1), (:,1), (r,1), (l,1), (s,3)...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combineByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//KVcharacters=>(s,1), (p,1), (a,1), (r,1), (k,1), (t,1), (h,1), (e,1),\n",
    "val valToCombiner = (value:Int) => List(value)// (value, 1) =>(key, (value, 1))\n",
    "val mergeValuesFunc = (vals:List[Int], valToAppend:Int) => valToAppend :: vals // (key, (value, 1)) =>(key, (total, count)) in every partition\n",
    "val mergeCombinerFunc = (vals1:List[Int], vals2:List[Int]) => vals1 ::: vals2//(key, (total, count)) => (key, (totalAcrossAllPartitions, countAcrossAllPartitions))\n",
    "val outputPartitions = 6\n",
    "KVcharacters.combineByKey(valToCombiner,mergeValuesFunc,mergeCombinerFunc,outputPartitions).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### foldByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KVcharacters.foldByKey(0)(addFunc).collect() //merges the values for each key with neutral “zero value,” 0 for +, or 1 for *\n",
    "//Array((d,4), (p,3), (t,3), (b,1), (h,1), (n,2), (f,1), (v,1), (:,1),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeCombinerFunc(mergeValuesFunc(valToCombiner(10),5),mergeValuesFunc(valToCombiner(10),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//CoGroups give you the ability to group together up to three key–value RDDs together in Scala\n",
    "import scala.util.Random\n",
    "val distinctChars = words.flatMap(word => word.toLowerCase.toSeq).distinct\n",
    "val charRDD = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "val charRDD2 = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "val charRDD3 = distinctChars.map(c => (c, new Random().nextDouble()))\n",
    "charRDD.cogroup(charRDD2, charRDD3).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//coalesce effectively collapses partitions on the same worker in order to avoid a shuffle of the data when repartitioning.\n",
    "words.coalesce(1).getNumPartitions // 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//The repartition operation allows you to repartition your data up or down but performs a shuffle across nodes in the process.\n",
    "words.repartition(10) // gives us 10 partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repartitionAndSortWithinPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This operation gives you the ability to repartition as well as specify the ordering of each one of those\n",
    "output partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".csv(\"/home/koushik/git/spark/input/Spark-The-Definitive-Guide/retail-data/all/\")\n",
    "val rdd = df.coalesce(10).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val keyedRDD = rdd.keyBy(row => row(6).asInstanceOf[Int].toDouble)\n",
    "keyedRDD.partitionBy(new HashPartitioner(10)).take(10)\n",
    "val groupRDD=keyedRDD.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd.map(r => r(0)).take(1).foreach(println) //groupByKey\n",
    "keyedRDD.take(1).foreach(println)//(17850.0,[536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom])\n",
    "groupRDD.take(5).foreach(println)//\n",
    "println(keyedRDD.count)//541909\n",
    "println(groupRDD.count)//4373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.Partitioner\n",
    "class DomainPartitioner extends Partitioner {\n",
    "def numPartitions = 3\n",
    "def getPartition(key: Any): Int = {\n",
    "val customerId = key.asInstanceOf[Double].toInt\n",
    "if (customerId == 17850.0 || customerId == 12583.0 || customerId == 13927.0  || customerId == 12853.0) {\n",
    "return 0\n",
    "} else {\n",
    "return new java.util.Random().nextInt(2) + 1\n",
    "}\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(4, 4298, 4298)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyedRDD\n",
    ".partitionBy(new DomainPartitioner).map(_._1).glom().map(_.toSet.toSeq.length)\n",
    ".take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
