{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DataFrameReader.format(...).option(\"key\", \"value\").schema(...).load()\n",
    "\n",
    "We access DataFrameReader through the SparkSession via the read attribute\n",
    "\n",
    "spark.read.format(\"csv\")\n",
    ".option(\"mode\", \"FAILFAST\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".option(\"path\", \"path/to/file(s)\")\n",
    ".schema(someSchema)\n",
    ".load()\n",
    "\n",
    "read modes \n",
    "permissive    -> Sets all fields to null when it encounters a corrupted record, places all corrupted records in a string column\n",
    "called _corrupt_record\n",
    "dropMalformed -> Drops the row that contains malformed records\n",
    "failFast      -> Fails immediately upon encountering malformed records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()\n",
    "\n",
    "dataframe.write.format(\"csv\")\n",
    ".option(\"mode\", \"OVERWRITE\")\n",
    ".option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    ".option(\"path\", \"path/to/file(s)\")\n",
    ".save()\n",
    "\n",
    "write modes\n",
    "append        -> Appends the output files to the list of files that already exist at that location\n",
    "overwrite     -> Will completely overwrite any data that already exists there\n",
    "errorIfExists -> Throws an error and fails the write if data or files already exist at the specified location\n",
    "ignore        -> If data or files exist at the location, do nothing with the current DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myManualSchema = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,false))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,false))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "val myManualSchema = new StructType(Array(\n",
    "new StructField(\"DEST_COUNTRY_NAME\", StringType, true),\n",
    "new StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\n",
    "new StructField(\"count\", LongType, false)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csvFile = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val csvFile=spark.read.format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"mode\", \"FAILFAST\")\n",
    ".schema(myManualSchema)\n",
    ".load(\"/home/koushik/git/spark/input/Spark-The-Definitive-Guide/flight-data/csv/2010-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvFile.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "//write\n",
    "csvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\"/home/koushik/git/spark/output/Spark-The-Definitive-Guide/flight-data/csv/2010-summary.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jsonFile = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsonFile=spark.read\n",
    ".format(\"json\")\n",
    ".option(\"mode\", \"FAILFAST\")\n",
    ".option(\"multiLine\", false)\n",
    ".schema(myManualSchema)\n",
    ".load(\"/home/koushik/git/spark/input/Spark-The-Definitive-Guide/flight-data/json/2010-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonFile.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"json\").mode(\"overwrite\")\n",
    ".save(\"/home/koushik/git/spark/output/Spark-The-Definitive-Guide/flight-data/json/2010-summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parquetFile = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parquetFile=spark.read.format(\"parquet\")\n",
    ".load(\"/home/koushik/git/spark/input/Spark-The-Definitive-Guide/flight-data/parquet/2010-summary.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\n",
    ".save(\"/home/koushik/git/spark/output/Spark-The-Definitive-Guide/flight-data/parquet/2010-summary.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orcFile = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orcFile=spark.read.format(\"orc\").load(\"/home/koushik/git/spark/input/Spark-The-Definitive-Guide/flight-data/orc/2010-summary.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:26: error: not found: value orcFile\n",
       "       orcFile.show(2)\n",
       "       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orcFile.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"orc\").mode(\"overwrite\").save(\"/home/koushik/git/spark/output/Spark-The-Definitive-Guide/flight-data/orc/2010-summary.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dbDataFrame = [emp_no: int, birth_date: date ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[emp_no: int, birth_date: date ... 4 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dbDataFrame = spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:mysql://127.0.0.1:3306?useSSL=false\")\n",
    "    .option(\"dbtable\",\"employees.employees\")\n",
    "    .option(\"user\", \"root\")\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "    //.option(\"password\", \"root\")\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10040|1959-09-13|     Weiyi|  Meriste|     F|1993-02-14|\n",
      "| 10268|1958-06-03|    Nishit|   Siochi|     M|1986-12-17|\n",
      "| 10288|1959-06-02|    Selwyn|    Perri|     M|1994-08-29|\n",
      "| 10463|1954-05-20|       Ung|   Zaiane|     M|1987-12-25|\n",
      "| 10677|1963-02-20| Alejandra|    Perng|     F|1996-10-07|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame.select(\"*\").distinct().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan JDBCRelation(employees.employees) [numPartitions=1] [emp_no#23,birth_date#24,first_name#25,last_name#26,gender#27,hire_date#28] PushedFilters: [*IsNotNull(gender), *EqualTo(gender,M)], ReadSchema: struct<emp_no:int,birth_date:date,first_name:string,last_name:string,gender:string,hire_date:date>\n"
     ]
    }
   ],
   "source": [
    "//Query Pushdown . Gets only the resultant data from jdbc\n",
    "dbDataFrame.filter(\"gender ='M'\").explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pushdownQuery = (select * from employees where gender='F' AND birth_date > '1965-01-01' and hire_date < '1986-01-01')AS flight_info\n",
       "dbDataFrame = [emp_no: int, birth_date: date ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[emp_no: int, birth_date: date ... 4 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Passing the whole query\n",
    "val pushdownQuery = \"\"\"(select * from employees where gender='F' AND birth_date > '1965-01-01' and hire_date < '1986-01-01')AS flight_info\"\"\"\n",
    "val dbDataFrame = spark.read.format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:mysql://127.0.0.1:3306?useSSL=false\")\n",
    "    .option(\"dbtable\",\"employees.employees\")\n",
    "    .option(\"user\", \"root\")\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbDataFrame.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dbDataFramePart = [emp_no: int, birth_date: date ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[emp_no: int, birth_date: date ... 4 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dbDataFramePart = spark.read.format(\"jdbc\")\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "    .option(\"url\", \"jdbc:mysql://127.0.0.1:3306?useSSL=false\")\n",
    "    .option(\"dbtable\",\"employees.employees\")\n",
    "    .option(\"user\", \"root\")    \n",
    "    .option(\"numPartitions\", 10)// Reading from databases in parallel\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "props = {user=root, driver=com.mysql.jdbc.Driver}\n",
       "predicates = Array(hire_date < '1986-01-01' OR birth_date > '1965-01-01', hire_date < '1980-01-01' OR birth_date > '1960-01-01')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(hire_date < '1986-01-01' OR birth_date > '1965-01-01', hire_date < '1980-01-01' OR birth_date > '1960-01-01')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val props = new java.util.Properties\n",
    "props.setProperty(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "//props.setProperty(\"url\", \"jdbc:mysql://127.0.0.1:3306?useSSL=false\")\n",
    "//props.setProperty(\"dbtable\",\"employees.dept_emp\")\n",
    "props.setProperty(\"user\", \"root\")\n",
    "val predicates = Array(\n",
    "\"hire_date < '1986-01-01' OR birth_date > '1965-01-01'\",\n",
    "\"hire_date < '1980-01-01' OR birth_date > '1960-01-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+\n",
      "|emp_no|dept_no| from_date|   to_date|\n",
      "+------+-------+----------+----------+\n",
      "| 10001|   d005|1986-06-26|9999-01-01|\n",
      "| 10002|   d007|1996-08-03|9999-01-01|\n",
      "+------+-------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(\"jdbc:mysql://127.0.0.1:3306?useSSL=false\",\"employees.dept_emp\", props).show(2)\n",
    "spark.read.jdbc(\"jdbc:mysql://127.0.0.1:3306?useSSL=false\",\"employees.dept_emp\", props).rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colName = emp_no\n",
       "lowerBound = 0\n",
       "upperBound = 999999\n",
       "numPartitions = 10\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val colName = \"emp_no\"\n",
    "val lowerBound = 0L\n",
    "val upperBound = 999999L // this is the max count in our database\n",
    "val numPartitions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.jdbc(\"jdbc:mysql://127.0.0.1:3306?useSSL=false\",\"employees.dept_emp\",colName,lowerBound,upperBound,\n",
    "                numPartitions,props).count()\n",
    "spark.read.jdbc(\"jdbc:mysql://127.0.0.1:3306?useSSL=false\",\"employees.dept_emp\",colName,lowerBound,upperBound,\n",
    "                numPartitions,props).rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                rows|\n",
      "+--------------------+\n",
      "|[DEST_COUNTRY_NAM...|\n",
      "|[United States, R...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.textFile(\"/home/koushik/git/spark/input/Spark-The-Definitive-Guide/flight-data/csv/2010-summary.csv\")\n",
    ".selectExpr(\"split(value, ',') as rows\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.select(\"DEST_COUNTRY_NAME\").\n",
    "write.text(\"/home/koushik/git/spark/output/Spark-The-Definitive-Guide/flight-data/csv/2010-summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.limit(10).write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\")\n",
    ".save(\"/home/koushik/git/spark/output/Spark-The-Definitive-Guide/flight-data/partitionedcsv/2010-summary.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.format(\"parquet\").mode(\"overwrite\")\n",
    ".bucketBy(10, \"count\").saveAsTable(\"bucketedFiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
