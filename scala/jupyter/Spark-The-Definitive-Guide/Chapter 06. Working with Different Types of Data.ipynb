{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".load(\"/home/koushik/git/spark/input/Spark-The-Definitive-Guide/retail-data/by-day/2010-12-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// .equalTo is same as \"===\"\n",
    "import org.apache.spark.sql.functions.col\n",
    "df.where(col(\"InvoiceNo\").equalTo(536365))\n",
    "//df.where(col(\"InvoiceNo\")===(536365))\n",
    ".select(\"InvoiceNo\", \"Description\")\n",
    ".show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col(\"InvoiceNo\") === 536365)\n",
    ".select(\"InvoiceNo\", \"Description\")\n",
    ".show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Boolean expression, combining filters\n",
    "val priceFilter = col(\"UnitPrice\") > 600\n",
    "val descripFilter = col(\"Description\").contains(\"POSTAGE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col(\"StockCode\").isin(\"DOT\",\"COM\"))\n",
    ".where(priceFilter.or(descripFilter))\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val DOTCodeFilter = col(\"StockCode\") === \"DOT\"\n",
    "val priceFilter = col(\"UnitPrice\") > 600\n",
    "val descripFilter = col(\"Description\").contains(\"POSTAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"isExpensive\",DOTCodeFilter.and(priceFilter.or(descripFilter))).show(5)\n",
    "//.where(\"isExpensive\").select(\"unitPrice\", \"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"isExpensive\",DOTCodeFilter.and(priceFilter.or(descripFilter)))\n",
    ".where(\"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"isExpensive\",DOTCodeFilter.and(priceFilter.or(descripFilter)))\n",
    ".where(\"isExpensive\").select(\"unitPrice\", \"isExpensive\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Numbers: POW function\n",
    "import org.apache.spark.sql.functions.{expr, pow}\n",
    "val fabricatedQuantity =pow(col(\"Quantity\") * col(\"UnitPrice\"),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|234.08999999999997|\n",
      "|   17850.0|          413.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"CustomerId\"),fabricatedQuantity.alias(\"realQuantity\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"CustomerId\",\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|rounded|UnitPrice|\n",
      "+-------+---------+\n",
      "|    2.6|     2.55|\n",
      "|    3.4|     3.39|\n",
      "|    2.8|     2.75|\n",
      "|    3.4|     3.39|\n",
      "|    3.4|     3.39|\n",
      "+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//rounding off\n",
    "import org.apache.spark.sql.functions.{round, bround}\n",
    "df.select(round(col(\"UnitPrice\"), 1).alias(\"rounded\"),col(\"UnitPrice\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|\n",
      "| stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//\"describe\" method will take all numeric columns and calculate the count, mean, standard deviation, min, and max.\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|initcap(Description)              |\n",
      "+----------------------------------+\n",
      "|White Hanging Heart T-light Holder|\n",
      "|White Metal Lantern               |\n",
      "+----------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//turn into capital letters \n",
    "import org.apache.spark.sql.functions.{initcap}\n",
    "df.select(initcap(col(\"Description\"))).show(2, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------------+\n",
      "|         Description|  lower(Description)|upper(lower(Description))|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{lower, upper}\n",
    "df.select(col(\"Description\"),lower(col(\"Description\")),upper(lower(col(\"Description\")))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+---+----------+\n",
      "| ltrim| rtrim| trim| lp|        rp|\n",
      "+------+------+-----+---+----------+\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "+------+------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{lit, ltrim, rtrim, rpad,trim,lpad}\n",
    "df.select(\n",
    "ltrim(lit(\" HELLO \")).as(\"ltrim\"),\n",
    "rtrim(lit(\" HELLO \")).as(\"rtrim\"),\n",
    "trim(lit(\" HELLO \")).as(\"trim\"),\n",
    "lpad(lit(\"HELLO\"), 3, \" \").as(\"lp\"),\n",
    "rpad(lit(\"HELLO\"), 10, \" \").as(\"rp\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.regexp_replace\n",
    "val simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\n",
    "val regexString = simpleColors.map(_.toUpperCase).mkString(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|       color_cleaned|         Description|\n",
      "+--------------------+--------------------+\n",
      "|COLOR HANGING HEA...|WHITE HANGING HEA...|\n",
      "| COLOR METAL LANTERN| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    regexp_replace(col(\"Description\"),regexString, \"COLOR\")\n",
    "    .alias(\"color_cleaned\"),col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------+\n",
      "|translate(Description, LEET, 1337)|         Description|\n",
      "+----------------------------------+--------------------+\n",
      "|              WHI73 HANGING H3A...|WHITE HANGING HEA...|\n",
      "|               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|\n",
      "+----------------------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//replace given characters with other characters.\n",
    "import org.apache.spark.sql.functions.translate\n",
    "df.select(translate(col(\"Description\"), \"LEET\", \"1337\"),col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.regexp_extract\n",
    "val regexString = simpleColors.map(_.toUpperCase).mkString(\"(\", \"|\", \")\")\n",
    "// the | signifies OR in regular expression syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|color_cleaned|         Description|\n",
      "+-------------+--------------------+\n",
      "|        WHITE|WHITE HANGING HEA...|\n",
      "|        WHITE| WHITE METAL LANTERN|\n",
      "+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(regexp_extract(col(\"Description\"), regexString, 1).alias(\"color_cleaned\"),col(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//contains\n",
    "val containsBlack = col(\"Description\").contains(\"BLACK\")\n",
    "val containsWhite = col(\"DESCRIPTION\").contains(\"WHITE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"hasSimpleColor\", containsBlack.or(containsWhite)).filter(\"hasSimpleColor\").select(\"Description\").show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// contains with a number of items /varargs\n",
    "val simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\n",
    "val selectedColumns = simpleColors.map(color => {col(\"Description\").contains(color.toUpperCase).alias(s\"is_$color\")}):+expr(\"*\")// could also append this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(selectedColumns:_*).where(col(\"is_white\").or(col(\"is_red\"))).select(\"Description\").show(3, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{current_date, current_timestamp}\n",
    "val dateDF = spark.range(10).withColumn(\"today\", current_date()).withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|     today|                 now|\n",
      "+---+----------+--------------------+\n",
      "|  0|2019-02-07|2019-02-07 02:58:...|\n",
      "|  1|2019-02-07|2019-02-07 02:58:...|\n",
      "+---+----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2019-02-02|        2019-02-12|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//add and subtract\n",
    "import org.apache.spark.sql.functions.{date_add, date_sub}\n",
    "dateDF.select(date_sub(col(\"today\"), 5),date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+----------+\n",
      "| id|     today|                 now|  week_ago|\n",
      "+---+----------+--------------------+----------+\n",
      "|  0|2019-02-07|2019-02-07 02:58:...|2019-01-31|\n",
      "+---+----------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//difference between two dates. \"datediff\"\n",
    "import org.apache.spark.sql.functions.{datediff, months_between,to_date}\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7)).show(1)\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7)).select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//to_date\n",
    "val start_end=dateDF.select(to_date(lit(\"2016-01-01\")).alias(\"start\"),to_date(lit(\"2017-05-22\")).alias(\"end\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     start|       end|\n",
      "+----------+----------+\n",
      "|2016-01-01|2017-05-22|\n",
      "+----------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_end.show(1)\n",
    "start_end.select(months_between(col(\"start\"), col(\"end\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2017-01-01|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{to_date, lit}\n",
    "spark.range(5).withColumn(\"date\", lit(\"2017-01-01\")).select(to_date(col(\"date\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|to_date('2016-20-12')|to_date('2017-12-11')|\n",
      "+---------------------+---------------------+\n",
      "|                 null|           2017-12-11|\n",
      "+---------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Format must be of YYYY-MM-DD\n",
    "dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// we added an extra optional argument  dateFormat\" to \"to_date\" function to change from  YYYY-DD-MM to  YYYY-MM-DD\n",
    "import org.apache.spark.sql.functions.to_date\n",
    "val dateFormat = \"yyyy-dd-MM\"\n",
    "val cleanDateDF = spark.range(1).select(to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDateDF.createOrReplaceTempView(\"dateTable2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               date|\n",
      "+-------------------+\n",
      "|2017-11-12 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//  \"to_timestamp\"  always needs a format\n",
    "import org.apache.spark.sql.functions.to_timestamp\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat).alias(\"date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n",
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//comparing between date or timestamp  when they are in  correct format and type\n",
    "cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()\n",
    "cleanDateDF.filter(col(\"date2\") > \"'2017-12-12'\").show()//as a string, which Spark parses to a literal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|         Description|CustomerId|\n",
      "+--------------------+----------+\n",
      "|WHITE HANGING HEA...|   17850.0|\n",
      "| WHITE METAL LANTERN|   17850.0|\n",
      "+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------------------------------+\n",
      "|coalesce(Description, CustomerId)|\n",
      "+---------------------------------+\n",
      "|             WHITE HANGING HEA...|\n",
      "|              WHITE METAL LANTERN|\n",
      "+---------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Coalesce: function  allows you to select the first non-null value from a set of columns.\n",
    "//In this case, there are no null values, so it simply returns the first column:\n",
    "import org.apache.spark.sql.functions.coalesce\n",
    "df.select(col(\"Description\"), col(\"CustomerId\")).show(2)\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//drop: removes rows that contain nulls. \n",
    "df.na.drop()\n",
    "df.na.drop(\"all\")//drop if all values of a row are null\n",
    "df.na.drop(\"any\")//drop if any value of a row is null\n",
    "df.na.drop(\"all\", Seq(\"StockCode\", \"InvoiceNo\"))//only  for selected columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//fill : you can fill one or more columns with a set of values.\n",
    "df.na.fill(\"All Null values become this string\")\n",
    "df.na.fill(5)// replace all null values of any  numeric  column with 5\n",
    "df.na.fill(5, Seq(\"StockCode\", \"InvoiceNo\"))// replace all null values of specific columns column with 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Same as above with a map\n",
    "val fillColValues = Map(\"StockCode\" -> 5, \"Description\" -> \"No Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.na.fill(fillColValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.replace(\"Description\", Map(\"\" -> \"UNKNOWN\"))// replace empty string \"\" with \"UNKNOWN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//use asc_nulls_first, desc_nulls_first, asc_nulls_last, or desc_nulls_last \n",
    "//to specify where you would like your null values to appear in an ordered DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|             complex|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+--------------------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|[WHITE HANGING HE...|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|[WHITE METAL LANT...|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+--------------------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|             complex|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+--------------------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|[WHITE HANGING HE...|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|[WHITE METAL LANT...|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+--------------------+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\").show(2)\n",
    "df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// A df ( named complex) within a df\n",
    "import org.apache.spark.sql.functions.struct\n",
    "val complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         Description|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+\n",
      "| complex.Description|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+---------+\n",
      "|         Description|InvoiceNo|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|   536365|\n",
      "| WHITE METAL LANTERN|   536365|\n",
      "+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.createOrReplaceTempView(\"complexDFSchema\")\n",
    "complexDF.select(\"complex.Description\").show(2)// accessing data from the inner df\n",
    "complexDF.select(col(\"complex\").getField(\"Description\")).show(2)// accessing data from the inner df\n",
    "complexDF.select(\"complex.*\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|split(Description,  )|\n",
      "+---------------------+\n",
      "| [WHITE, HANGING, ...|\n",
      "| [WHITE, METAL, LA...|\n",
      "+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//split\n",
    "import org.apache.spark.sql.functions.split\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\")).selectExpr(\"array_col[0]\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|size(split(Description,  ))|\n",
      "+---------------------------+\n",
      "|                          5|\n",
      "|                          3|\n",
      "+---------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Size/Length\n",
    "import org.apache.spark.sql.functions.size\n",
    "df.select(size(split(col(\"Description\"), \" \"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|array_contains(split(Description,  ), WHITE)|\n",
      "+--------------------------------------------+\n",
      "|                                        true|\n",
      "|                                        true|\n",
      "+--------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//Contains\n",
    "// in Scala\n",
    "import org.apache.spark.sql.functions.array_contains\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------+\n",
      "|InvoiceNo|         Description|            splitted|exploded|\n",
      "+---------+--------------------+--------------------+--------+\n",
      "|   536365|WHITE HANGING HEA...|[WHITE, HANGING, ...|   WHITE|\n",
      "|   536365|WHITE HANGING HEA...|[WHITE, HANGING, ...| HANGING|\n",
      "|   536365|WHITE HANGING HEA...|[WHITE, HANGING, ...|   HEART|\n",
      "|   536365|WHITE HANGING HEA...|[WHITE, HANGING, ...| T-LIGHT|\n",
      "|   536365|WHITE HANGING HEA...|[WHITE, HANGING, ...|  HOLDER|\n",
      "+---------+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//explode\n",
    "import org.apache.spark.sql.functions.{split, explode}\n",
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \")).withColumn(\"exploded\", explode(col(\"splitted\")))\n",
    ".select( \"InvoiceNo\",\"Description\",\"splitted\", \"exploded\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         complex_map|\n",
      "+--------------------+\n",
      "|[WHITE HANGING HE...|\n",
      "|[WHITE METAL LANT...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.map\n",
    "df.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|   key|               value|\n",
      "+------+--------------------+\n",
      "|536365|WHITE HANGING HEA...|\n",
      "|536365| WHITE METAL LANTERN|\n",
      "+------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(map( col(\"InvoiceNo\"),col(\"Description\")).alias(\"complex_map\"))\n",
    ".selectExpr(\"explode(complex_map)\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val jsonDF = spark.range(1).selectExpr(\"\"\"'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|column|               tuple|\n",
      "+------+--------------------+\n",
      "|     2|{\"myJSONValue\":[1...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//get_json_object ,json_tuple\n",
    "import org.apache.spark.sql.functions.{get_json_object, json_tuple}\n",
    "jsonDF.select(get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\") as \"column\",\n",
    "                json_tuple(col(\"jsonString\"), \"myJSONKey\")as \"tuple\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|structstojson(myStruct)|\n",
      "+-----------------------+\n",
      "|   {\"InvoiceNo\":\"536...|\n",
      "|   {\"InvoiceNo\":\"536...|\n",
      "+-----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//to_json (from Struct to Json)\n",
    "import org.apache.spark.sql.functions.to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\").select(to_json(col(\"myStruct\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//from_json (from Json to specified schema)\n",
    "import org.apache.spark.sql.functions.from_json\n",
    "import org.apache.spark.sql.types._\n",
    "val parseSchema = new StructType(Array(\n",
    "        new StructField(\"InvoiceNo\",StringType,true),\n",
    "        new StructField(\"Description\",StringType,true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------+\n",
      "|jsontostructs(newJSON)|             newJSON|\n",
      "+----------------------+--------------------+\n",
      "|  [536365, WHITE HA...|{\"InvoiceNo\":\"536...|\n",
      "|  [536365, WHITE ME...|{\"InvoiceNo\":\"536...|\n",
      "+----------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\n",
    "       .select(to_json(col(\"myStruct\")).alias(\"newJSON\")).select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "def power3(number:Double):Double = number * number * number //define UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "println(power3(2.0))//use UDF but not on DF yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"power3\", power3(_:Double):Double)//register the UDF as a Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|cube|\n",
      "+----+\n",
      "|27.0|\n",
      "|64.0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExampleDF.selectExpr(\"power3(num) as cube\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val power3udf = udf(power3(_:Double):Double)\n",
    "val udfExampleDF = spark.range(3,5).toDF(\"num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|UDF(num)|\n",
      "+--------+\n",
      "|    27.0|\n",
      "|    64.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExampleDF.select(power3udf(col(\"num\"))).show(2)//use the UDF as any other function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
